{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npPjiU2vbILm"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch-geometric yfinance pyngrok streamlit plotly\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import pickle\n",
        "import warnings\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error\n",
        "\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "from datasets import Dataset\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "SEED = 42\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "print(f\"Environment Ready. Processing on: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sijIX4LMhPSr"
      },
      "outputs": [],
      "source": [
        "API_KEY = \"PASTE_YOUR_POLYGON_KEY_HERE\"\n",
        "TICKER = \"NVDA\"\n",
        "DATE_RANGES = [(\"2022-09-01\", \"2023-09-01\"), (\"2023-09-02\", \"2024-09-01\"), (\"2024-09-02\", \"2025-09-01\")]\n",
        "NEWS_FILENAME = f\"{TICKER}_News_Raw.csv\"\n",
        "\n",
        "def fetch_news_data():\n",
        "    if \"PASTE\" in API_KEY:\n",
        "        print(\"Skipping download: No API Key provided.\")\n",
        "        return\n",
        "\n",
        "    all_articles = []\n",
        "    base_url = \"https://api.polygon.io/v2/reference/news\"\n",
        "\n",
        "    for start, end in DATE_RANGES:\n",
        "        print(f\"Fetching news: {start} to {end}...\")\n",
        "        params = {\n",
        "            \"ticker\": TICKER, \"published_utc.gte\": start, \"published_utc.lte\": end,\n",
        "            \"limit\": 1000, \"sort\": \"published_utc\", \"order\": \"desc\", \"apiKey\": API_KEY\n",
        "        }\n",
        "\n",
        "        current_url = base_url\n",
        "        while True:\n",
        "            resp = requests.get(current_url, params=params if current_url == base_url else None)\n",
        "            if resp.status_code == 429:\n",
        "                time.sleep(60)\n",
        "                continue\n",
        "            if resp.status_code != 200:\n",
        "                break\n",
        "\n",
        "            data = resp.json()\n",
        "            all_articles.extend(data.get('results', []))\n",
        "\n",
        "            if 'next_url' in data:\n",
        "                current_url = data['next_url'] + f\"&apiKey={API_KEY}\"\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    if all_articles:\n",
        "        df = pd.DataFrame(all_articles)\n",
        "        df['Date'] = pd.to_datetime(df['published_utc']).dt.date\n",
        "        df = df.groupby('Date').head(30)\n",
        "\n",
        "        df_clean = df[['Date', 'title', 'article_url', 'description']].rename(columns={\n",
        "            'title': 'Headline', 'article_url': 'Link', 'description': 'Summary'\n",
        "        })\n",
        "        df_clean.fillna(\"No Data\", inplace=True)\n",
        "        df_clean.to_csv(NEWS_FILENAME, index=False)\n",
        "        print(f\"Saved {len(df_clean)} articles to {NEWS_FILENAME}\")\n",
        "\n",
        "if not os.path.exists(NEWS_FILENAME):\n",
        "    fetch_news_data()\n",
        "else:\n",
        "    print(f\"Data file {NEWS_FILENAME} already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX2LIWDSxffE"
      },
      "outputs": [],
      "source": [
        "PROCESSED_FILE = \"NVDA_Processed_Features.csv\"\n",
        "\n",
        "def process_nlp():\n",
        "    if not os.path.exists(NEWS_FILENAME):\n",
        "        print(\"Raw news file not found.\")\n",
        "        return\n",
        "\n",
        "    print(\"Initializing NLP Models...\")\n",
        "    df = pd.read_csv(NEWS_FILENAME).dropna()\n",
        "    df['AI_Text'] = df['Headline'] + \". \" + df['Summary']\n",
        "\n",
        "    dataset = Dataset.from_pandas(df[['AI_Text']])\n",
        "\n",
        "    sent_pipe = pipeline(\"text-classification\", model=\"ProsusAI/finbert\", device=0 if torch.cuda.is_available() else -1, truncation=True, max_length=512)\n",
        "    rel_pipe = pipeline(\"zero-shot-classification\", model=\"valhalla/distilbart-mnli-12-3\", device=0 if torch.cuda.is_available() else -1)\n",
        "    RELATION_LABELS = [\"Competitor\", \"Supplier\", \"Customer\", \"Partner\", \"Regulatory\", \"Neutral\"]\n",
        "\n",
        "    print(f\"Processing {len(df)} articles...\")\n",
        "\n",
        "    sent_scores = []\n",
        "    for out in tqdm(sent_pipe(KeyDataset(dataset, \"AI_Text\"), batch_size=64), total=len(df), desc=\"Sentiment\"):\n",
        "        score = out['score'] if out['label'] == 'positive' else -out['score'] if out['label'] == 'negative' else 0.0\n",
        "        sent_scores.append(score)\n",
        "\n",
        "    rel_types = []\n",
        "    for out in tqdm(rel_pipe(KeyDataset(dataset, \"AI_Text\"), candidate_labels=RELATION_LABELS, batch_size=64), total=len(df), desc=\"Relations\"):\n",
        "        rel_types.append(out['labels'][0])\n",
        "\n",
        "    df['Sentiment_Score'] = sent_scores\n",
        "    df['Relation_Type'] = rel_types\n",
        "\n",
        "    df[['Date', 'Headline', 'Sentiment_Score', 'Relation_Type']].to_csv(PROCESSED_FILE, index=False)\n",
        "    print(f\"NLP Processing Complete. Saved to {PROCESSED_FILE}\")\n",
        "\n",
        "if not os.path.exists(PROCESSED_FILE) and os.path.exists(NEWS_FILENAME):\n",
        "    process_nlp()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub5qBWihWjnJ"
      },
      "outputs": [],
      "source": [
        "GRAPH_FILE = \"NVDA_Dynamic_Graph.pkl\"\n",
        "RELATION_MAP = {r: i for i, r in enumerate(['Ego', 'Competitor', 'Regulatory', 'Partner', 'Supplier', 'Customer', 'Neutral'])}\n",
        "\n",
        "def build_graphs():\n",
        "    if not os.path.exists(PROCESSED_FILE): return\n",
        "\n",
        "    print(\"Constructing Dynamic Knowledge Graphs...\")\n",
        "    df = pd.read_csv(PROCESSED_FILE)\n",
        "    grouped = df.groupby('Date')\n",
        "    dataset = []\n",
        "\n",
        "    for date, group in tqdm(grouped, desc=\"Building Daily Snapshots\"):\n",
        "        num_nodes = len(group) + 1\n",
        "        x = torch.zeros((num_nodes, len(RELATION_MAP) + 1), dtype=torch.float)\n",
        "        x[0, RELATION_MAP['Ego']] = 1.0\n",
        "\n",
        "        for i, (_, row) in enumerate(group.iterrows()):\n",
        "            node_idx = i + 1\n",
        "            rel_idx = RELATION_MAP.get(row['Relation_Type'], RELATION_MAP['Neutral'])\n",
        "            x[node_idx, rel_idx] = 1.0\n",
        "            x[node_idx, -1] = row['Sentiment_Score']\n",
        "\n",
        "        sources = torch.arange(1, num_nodes, dtype=torch.long)\n",
        "        targets = torch.zeros(len(group), dtype=torch.long)\n",
        "        edge_index = torch.stack([sources, targets], dim=0)\n",
        "\n",
        "        data = Data(x=x, edge_index=edge_index)\n",
        "        data.date = date\n",
        "        dataset.append(data)\n",
        "\n",
        "    with open(GRAPH_FILE, 'wb') as f:\n",
        "        pickle.dump(dataset, f)\n",
        "    print(f\"Graph serialization complete: {len(dataset)} snapshots.\")\n",
        "\n",
        "if not os.path.exists(GRAPH_FILE):\n",
        "    build_graphs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMM5lO5mz8EF"
      },
      "outputs": [],
      "source": [
        "class IntegratedTrader(nn.Module):\n",
        "    def __init__(self, node_feat_dim, kpi_dim, gnn_out=32, lstm_hidden=128):\n",
        "        super(IntegratedTrader, self).__init__()\n",
        "        self.gnn1 = GCNConv(node_feat_dim, 64)\n",
        "        self.gnn2 = GCNConv(64, gnn_out)\n",
        "        self.lstm = nn.LSTM(gnn_out + kpi_dim, lstm_hidden, batch_first=True, dropout=0.2)\n",
        "        self.head = nn.Linear(lstm_hidden, 1)\n",
        "\n",
        "    def forward(self, graph_list, kpi_tensor):\n",
        "        sentiment_vecs = []\n",
        "        for g in graph_list:\n",
        "            x, edge_index = g.x.to(DEVICE), g.edge_index.to(DEVICE)\n",
        "            x = F.relu(self.gnn1(x, edge_index))\n",
        "            x = self.gnn2(x, edge_index)\n",
        "            batch = torch.zeros(x.size(0), dtype=torch.long).to(DEVICE)\n",
        "            day_vec = global_mean_pool(x, batch)\n",
        "            sentiment_vecs.append(day_vec)\n",
        "\n",
        "        sentiment_seq = torch.stack(sentiment_vecs, dim=1)\n",
        "        fusion = torch.cat((sentiment_seq, kpi_tensor), dim=2)\n",
        "        lstm_out, _ = self.lstm(fusion)\n",
        "        return self.head(lstm_out[:, -1, :])\n",
        "\n",
        "class DirectionalLoss(nn.Module):\n",
        "    def __init__(self, penalty=2.0):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.penalty = penalty\n",
        "\n",
        "    def forward(self, pred, target, prev_price):\n",
        "        mse = self.mse(pred, target)\n",
        "        true_dir = torch.sign(target - prev_price)\n",
        "        pred_dir = torch.sign(pred - prev_price)\n",
        "        dir_mismatch = (true_dir != pred_dir).float()\n",
        "        return mse + (self.penalty * torch.mean(dir_mismatch * torch.abs(target - pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBev4f2nzrLM"
      },
      "outputs": [],
      "source": [
        "START_DATE = \"2022-09-01\"\n",
        "END_DATE = \"2025-09-02\"\n",
        "TRAIN_CUTOFF = \"2025-06-15\"\n",
        "SEQ_LEN = 5\n",
        "EPOCHS = 50\n",
        "\n",
        "def train_and_evaluate():\n",
        "    print(\"Fetching Market Data and Aligning Sequences...\")\n",
        "\n",
        "    with open(GRAPH_FILE, 'rb') as f:\n",
        "        raw_graphs = pickle.load(f)\n",
        "\n",
        "    graph_map = {pd.to_datetime(g.date).strftime('%Y-%m-%d'): Data(x=g.x, edge_index=g.edge_index) for g in raw_graphs}\n",
        "\n",
        "    df = yf.download(TICKER, start=START_DATE, end=END_DATE, progress=False)\n",
        "    if isinstance(df.columns, pd.MultiIndex): df.columns = [c[0] for c in df.columns]\n",
        "    df.reset_index(inplace=True)\n",
        "    df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "    df['SMA_14'] = df['Close'].rolling(14).mean()\n",
        "    df['MACD'] = df['Close'].ewm(span=12).mean() - df['Close'].ewm(span=26).mean()\n",
        "    df['Return'] = df['Close'].pct_change()\n",
        "    df['RSI'] = 100 - (100 / (1 + df['Close'].diff().clip(lower=0).rolling(14).mean() / -df['Close'].diff().clip(upper=0).rolling(14).mean()))\n",
        "    df['Target'] = df['Close'].shift(-1)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    train_df = df[df['Date'] <= TRAIN_CUTOFF].copy()\n",
        "    test_df = df[df['Date'] > TRAIN_CUTOFF].copy()\n",
        "\n",
        "    KPI_COLS = ['Close', 'RSI', 'MACD', 'SMA_14', 'Return', 'Target']\n",
        "    scaler = MinMaxScaler()\n",
        "    train_df[KPI_COLS] = scaler.fit_transform(train_df[KPI_COLS])\n",
        "    test_df[KPI_COLS] = scaler.transform(test_df[KPI_COLS])\n",
        "\n",
        "    full_df = pd.concat([train_df, test_df])\n",
        "    date_to_kpi = {r['Date']: r[KPI_COLS[:-1]].values.astype(float) for _, r in full_df.iterrows()}\n",
        "    date_to_target = {r['Date']: r['Target'] for _, r in full_df.iterrows()}\n",
        "\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    dates = []\n",
        "\n",
        "    common_dates = sorted(list(set(date_to_kpi.keys()) & set(graph_map.keys())))\n",
        "    for i in range(len(common_dates) - SEQ_LEN):\n",
        "        seq_dates = common_dates[i:i+SEQ_LEN]\n",
        "        tgt_date = seq_dates[-1]\n",
        "        sequences.append(([graph_map[d] for d in seq_dates], [date_to_kpi[d] for d in seq_dates]))\n",
        "        targets.append(date_to_target[tgt_date])\n",
        "        dates.append(tgt_date)\n",
        "\n",
        "    split_idx = len([d for d in dates if d <= TRAIN_CUTOFF])\n",
        "    train_seqs, test_seqs = sequences[:split_idx], sequences[split_idx:]\n",
        "    train_y, test_y = targets[:split_idx], targets[split_idx:]\n",
        "\n",
        "    sample_g = train_seqs[0][0][0]\n",
        "    model = IntegratedTrader(node_feat_dim=sample_g.x.shape[1], kpi_dim=len(KPI_COLS)-1).to(DEVICE)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    crit = DirectionalLoss()\n",
        "\n",
        "    print(f\"Starting Training ({len(train_seqs)} samples)...\")\n",
        "    model.train()\n",
        "    for ep in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
        "        for i in range(len(train_seqs)):\n",
        "            gs, kpis = train_seqs[i]\n",
        "            y_true = torch.tensor([[train_y[i]]], dtype=torch.float).to(DEVICE)\n",
        "            kpi_in = torch.tensor(np.array(kpis), dtype=torch.float).unsqueeze(0).to(DEVICE)\n",
        "            prev_price = kpi_in[:, -1, 0].unsqueeze(1)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            pred = model(gs, kpi_in)\n",
        "            loss = crit(pred, y_true, prev_price)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "    torch.save(model.state_dict(), \"NVDA_Sniper_Model.pth\")\n",
        "\n",
        "    print(\"Evaluating Performance...\")\n",
        "    model.eval()\n",
        "    preds, acts = [], []\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(test_seqs)):\n",
        "            gs, kpis = test_seqs[i]\n",
        "            kpi_in = torch.tensor(np.array(kpis), dtype=torch.float).unsqueeze(0).to(DEVICE)\n",
        "            preds.append(model(gs, kpi_in).item())\n",
        "            acts.append(test_y[i])\n",
        "\n",
        "    dummy = np.zeros((len(preds), len(KPI_COLS)))\n",
        "    dummy[:, -1] = preds\n",
        "    final_preds = scaler.inverse_transform(dummy)[:, -1]\n",
        "    dummy[:, -1] = acts\n",
        "    final_acts = scaler.inverse_transform(dummy)[:, -1]\n",
        "\n",
        "    mae = mean_absolute_error(final_acts, final_preds)\n",
        "\n",
        "    prev_prices_scaled = [seq[1][-1][0] for seq in test_seqs]\n",
        "    dummy[:, 0] = prev_prices_scaled\n",
        "    prev_prices_real = scaler.inverse_transform(dummy)[:, 0]\n",
        "\n",
        "    true_dir = (final_acts > prev_prices_real).astype(int)\n",
        "    pred_dir = (final_preds > prev_prices_real).astype(int)\n",
        "    acc = accuracy_score(true_dir, pred_dir)\n",
        "\n",
        "    print(f\"Results:\")\n",
        "    print(f\"MAE: ${mae:.2f}\")\n",
        "    print(f\"Directional Accuracy: {acc:.2%}\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(final_acts, label='Actual', color='black', alpha=0.7)\n",
        "    plt.plot(final_preds, label='AI Prediction', color='blue', linestyle='--')\n",
        "    plt.title(f\"NVDA Prediction vs Reality | Acc: {acc:.2%}\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "train_and_evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgvFrNhX1MJ9"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import streamlit as st\n",
        "import plotly.graph_objects as go\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "DEVICE = torch.device('cpu')\n",
        "TICKER = \"NVDA\"\n",
        "MODEL_FILE = \"NVDA_Sniper_Model.pth\"\n",
        "SEQ_LEN = 5\n",
        "\n",
        "class IntegratedTrader(nn.Module):\n",
        "    def __init__(self, node_feat_dim, kpi_dim, gnn_out=32, lstm_hidden=128):\n",
        "        super(IntegratedTrader, self).__init__()\n",
        "        self.gnn1 = GCNConv(node_feat_dim, 64)\n",
        "        self.gnn2 = GCNConv(64, gnn_out)\n",
        "        self.lstm = nn.LSTM(gnn_out + kpi_dim, lstm_hidden, batch_first=True, dropout=0.2)\n",
        "        self.head = nn.Linear(lstm_hidden, 1)\n",
        "\n",
        "    def forward(self, graph_list, kpi_tensor):\n",
        "        sentiment_vecs = []\n",
        "        for g in graph_list:\n",
        "            x, edge_index = g.x.to(DEVICE), g.edge_index.to(DEVICE)\n",
        "            x = F.relu(self.gnn1(x, edge_index))\n",
        "            x = self.gnn2(x, edge_index)\n",
        "            batch = torch.zeros(x.size(0), dtype=torch.long).to(DEVICE)\n",
        "            sentiment_vecs.append(global_mean_pool(x, batch))\n",
        "\n",
        "        fusion = torch.cat((torch.stack(sentiment_vecs, dim=1), kpi_tensor), dim=2)\n",
        "        return self.head(self.lstm(fusion)[0][:, -1, :])\n",
        "\n",
        "def get_data(end_date):\n",
        "    end_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
        "    start_dt = end_dt - timedelta(days=90)\n",
        "    df = yf.download(TICKER, start=start_dt, end=end_dt + timedelta(days=1), progress=False)\n",
        "    if isinstance(df.columns, pd.MultiIndex): df.columns = [c[0] for c in df.columns]\n",
        "    df.reset_index(inplace=True)\n",
        "    df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "    df['SMA_14'] = df['Close'].rolling(14).mean()\n",
        "    df['MACD'] = df['Close'].ewm(span=12).mean() - df['Close'].ewm(span=26).mean()\n",
        "    df['Return'] = df['Close'].pct_change()\n",
        "    delta = df['Close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(14).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
        "    df['RSI'] = 100 - (100 / (1 + gain/loss))\n",
        "\n",
        "    return df[df['Date'] <= end_date].copy()\n",
        "\n",
        "st.set_page_config(page_title=\"NVDA AI Agent\", layout=\"wide\")\n",
        "st.title(\"NVDA Graph-AI Trader\")\n",
        "\n",
        "date_in = st.sidebar.date_input(\"Target Date\", datetime.today())\n",
        "if st.sidebar.button(\"Run Prediction\"):\n",
        "    target_str = date_in.strftime('%Y-%m-%d')\n",
        "\n",
        "    with st.spinner(\"Analyzing Market Structure...\"):\n",
        "        df = get_data(target_str)\n",
        "        if len(df) < SEQ_LEN:\n",
        "            st.error(\"Insufficient Data.\")\n",
        "            st.stop()\n",
        "\n",
        "        KPI_COLS = ['Close', 'RSI', 'MACD', 'SMA_14', 'Return']\n",
        "        scaler = MinMaxScaler()\n",
        "        df_scaled = df.copy()\n",
        "        df_scaled[KPI_COLS] = scaler.fit_transform(df[KPI_COLS])\n",
        "\n",
        "        kpi_input = torch.tensor(df_scaled[KPI_COLS].tail(SEQ_LEN).values, dtype=torch.float).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        NODE_DIM = 8\n",
        "        x = torch.zeros((10, NODE_DIM)); x[:, 0] = 1.0\n",
        "        edge_index = torch.tensor([[0,1],[1,0]], dtype=torch.long)\n",
        "        graph_seq = [Data(x=x, edge_index=edge_index) for _ in range(SEQ_LEN)]\n",
        "\n",
        "        try:\n",
        "            model = IntegratedTrader(node_feat_dim=NODE_DIM, kpi_dim=len(KPI_COLS)).to(DEVICE)\n",
        "            if os.path.exists(MODEL_FILE):\n",
        "                model.load_state_dict(torch.load(MODEL_FILE, map_location=DEVICE))\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                pred = model(graph_seq, kpi_input).item()\n",
        "\n",
        "            dummy = np.zeros((1, len(KPI_COLS)))\n",
        "            dummy[0, 0] = pred\n",
        "            price = scaler.inverse_transform(dummy)[0,0]\n",
        "\n",
        "            last_close = df['Close'].iloc[-1]\n",
        "            pct_change = (price - last_close)/last_close * 100\n",
        "\n",
        "            col1, col2 = st.columns(2)\n",
        "            col1.metric(\"Previous Close\", f\"${last_close:.2f}\")\n",
        "            col2.metric(\"AI Prediction\", f\"${price:.2f}\", f\"{pct_change:.2f}%\")\n",
        "\n",
        "            fig = go.Figure()\n",
        "            fig.add_trace(go.Scatter(x=df['Date'].tail(30), y=df['Close'].tail(30), name=\"History\"))\n",
        "            fig.add_trace(go.Scatter(x=[target_str], y=[price], mode='markers', marker=dict(size=12, color='red'), name=\"AI Target\"))\n",
        "            st.plotly_chart(fig)\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Prediction Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CumEQMxxZeZ-"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q pyngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "!nohup streamlit run app.py --server.port 8501 &\n",
        "\n",
        "\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "print(f\" Dashboard Live at: {public_url}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}