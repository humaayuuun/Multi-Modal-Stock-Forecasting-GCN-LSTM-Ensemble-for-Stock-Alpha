{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npPjiU2vbILm"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import date, timedelta\n",
        "import time\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "API_KEY = \"RpaX4cXYXe5uxWJzOmZyxMwuj9k4CQlY\"\n",
        "TICKER = \"NVDA\"\n",
        "ARTICLE_CAP_PER_DAY = 30\n",
        "\n",
        "\n",
        "DATE_RANGES = [\n",
        "    (\"2022-09-01\", \"2023-09-01\"),\n",
        "    (\"2023-09-02\", \"2024-09-01\"),\n",
        "    (\"2024-09-02\", \"2025-09-01\")\n",
        "]\n",
        "\n",
        "print(f\" Starting 'Safe Mode' Fetch for {TICKER} (3-Year History)...\")\n",
        "print(\"   NOTE: We will sleep for 20 seconds between years to respect API limits.\")\n",
        "\n",
        "all_articles = []\n",
        "\n",
        "\n",
        "for start_date, end_date in DATE_RANGES:\n",
        "    print(f\"\\nüìÖ Fetching Batch: {start_date} to {end_date}...\")\n",
        "\n",
        "    base_url = \"https://api.polygon.io/v2/reference/news\"\n",
        "    params = {\n",
        "        \"ticker\": TICKER,\n",
        "        \"published_utc.gte\": start_date,\n",
        "        \"published_utc.lte\": end_date,\n",
        "        \"limit\": 1000,\n",
        "        \"sort\": \"published_utc\",\n",
        "        \"order\": \"desc\",\n",
        "        \"apiKey\": API_KEY\n",
        "    }\n",
        "\n",
        "    current_url = base_url\n",
        "    batch_articles = []\n",
        "\n",
        "    while True:\n",
        "        if current_url == base_url:\n",
        "            response = requests.get(current_url, params=params)\n",
        "        else:\n",
        "            response = requests.get(f\"{current_url}&apiKey={API_KEY}\")\n",
        "\n",
        "        if response.status_code == 429:\n",
        "            print(\"  Rate Limit Hit! Sleeping 60s...\")\n",
        "            time.sleep(60)\n",
        "            continue\n",
        "\n",
        "        data = response.json()\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            results = data.get('results', [])\n",
        "            batch_articles.extend(results)\n",
        "            print(f\"   -> Got {len(results)} articles (Batch Total: {len(batch_articles)})\")\n",
        "\n",
        "            if 'next_url' in data:\n",
        "                current_url = data['next_url']\n",
        "                time.sleep(0.5)\n",
        "            else:\n",
        "                break\n",
        "        else:\n",
        "            print(f\" Error: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "    all_articles.extend(batch_articles)\n",
        "    print(f\" Finished Year. Total Articles so far: {len(all_articles)}\")\n",
        "    print(\" Resting for 20 seconds to reset API quota...\")\n",
        "    time.sleep(20)\n",
        "\n",
        "\n",
        "if len(all_articles) > 0:\n",
        "    df = pd.DataFrame(all_articles)\n",
        "    df['Date'] = pd.to_datetime(df['published_utc']).dt.date\n",
        "\n",
        "\n",
        "    df_capped = df.groupby('Date').head(ARTICLE_CAP_PER_DAY).copy()\n",
        "\n",
        "\n",
        "    df_capped['Source'] = df_capped['publisher'].apply(lambda x: x.get('name') if isinstance(x, dict) else 'Unknown')\n",
        "    df_clean = df_capped[['Date', 'title', 'article_url', 'Source', 'description']].rename(columns={\n",
        "        'title': 'Headline', 'article_url': 'Link', 'description': 'Summary'\n",
        "    })\n",
        "\n",
        "\n",
        "    full_date_range = pd.date_range(start=\"2022-09-01\", end=\"2025-09-01\").date\n",
        "    df_dates = pd.DataFrame(full_date_range, columns=['Date'])\n",
        "\n",
        "    df_final = pd.merge(df_dates, df_clean, on='Date', how='left')\n",
        "    df_final['Headline'] = df_final['Headline'].fillna(\"No News\")\n",
        "    df_final['Summary'] = df_final['Summary'].fillna(\"Market Closed or Quiet Day\")\n",
        "    df_final['Source'] = df_final['Source'].fillna(\"None\")\n",
        "\n",
        "\n",
        "    filename = f\"{TICKER}_Sept_2022_to_Sept_2025_News_COMPLETE.csv\"\n",
        "    df_final.to_csv(filename, index=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"üéâ SUCCESS! Downloaded {len(df_final)} rows.\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "\n",
        "    with open(filename, 'rb') as f:\n",
        "        b64 = base64.b64encode(f.read()).decode()\n",
        "    payload = f'data:text/csv;base64,{b64}'\n",
        "    html = f'<a download=\"{filename}\" href=\"{payload}\" target=\"_blank\"><button style=\"background-color:#4CAF50;color:white;padding:10px;\">DOWNLOAD COMPLETE 3-YEAR CSV</button></a>'\n",
        "    display(HTML(html))\n",
        "else:\n",
        "    print(\" Failed to fetch any articles.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sijIX4LMhPSr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# STAGE 2: GPU-OPTIMIZED\n",
        "!pip install transformers torch pandas tqdm datasets\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "from datasets import Dataset\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "INPUT_FILE = \"NVDA_Sept_2022_to_Sept_2025_News_COMPLETE.csv\"\n",
        "OUTPUT_FILE = \"NVDA_3Year_Processed_Data.csv\"\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\" Hardware Check: {'GPU ONLINE' if device==0 else ' CPU ONLY'}\")\n",
        "\n",
        "if os.path.exists(INPUT_FILE):\n",
        "    df = pd.read_csv(INPUT_FILE)\n",
        "    df = df.dropna(subset=['Headline', 'Summary'])\n",
        "\n",
        "    df['Headline'] = df['Headline'].astype(str)\n",
        "    df['Summary'] = df['Summary'].astype(str)\n",
        "\n",
        "    print(f\" Loaded {len(df)} articles.\")\n",
        "else:\n",
        "    print(f\" Error: '{INPUT_FILE}' not found.\")\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "if not df.empty:\n",
        "    print(\"\\n Loading AI Models...\")\n",
        "\n",
        "    df['AI_Text'] = df['Headline'] + \". \" + df['Summary']\n",
        "\n",
        "    hf_dataset = Dataset.from_pandas(df[['AI_Text']])\n",
        "\n",
        "    sentiment_pipe = pipeline(\n",
        "        \"text-classification\",\n",
        "        model=\"ProsusAI/finbert\",\n",
        "        device=device,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    relation_pipe = pipeline(\n",
        "        \"zero-shot-classification\",\n",
        "        model=\"valhalla/distilbart-mnli-12-3\",\n",
        "        device=device,\n",
        "        batch_size=BATCH_SIZE\n",
        "    )\n",
        "    RELATION_LABELS = [\"Competitor\", \"Supplier\", \"Customer\", \"Partner\", \"Regulatory\", \"Neutral\"]\n",
        "\n",
        "    print(f\"\\n‚ö° Streaming {len(df)} articles to GPU...\")\n",
        "\n",
        "    print(\"   ... Calculating Sentiment\")\n",
        "    sent_scores = []\n",
        "\n",
        "    for out in tqdm(sentiment_pipe(KeyDataset(hf_dataset, \"AI_Text\")), total=len(df)):\n",
        "        score = out['score'] if out['label'] == 'positive' else -out['score'] if out['label'] == 'negative' else 0.0\n",
        "        sent_scores.append(score)\n",
        "\n",
        "    print(\"   ... Calculating Relationships\")\n",
        "    rel_types = []\n",
        "\n",
        "    for out in tqdm(relation_pipe(KeyDataset(hf_dataset, \"AI_Text\"), candidate_labels=RELATION_LABELS), total=len(df)):\n",
        "        rel_types.append(out['labels'][0])\n",
        "\n",
        "    df['Sentiment_Score'] = sent_scores\n",
        "    df['Relation_Type'] = rel_types\n",
        "\n",
        "    df_final = df[['Date', 'Headline', 'Sentiment_Score', 'Relation_Type']]\n",
        "    df_final.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "    print(f\"\\n SUCCESS! Processed 3 Years of Data.\")\n",
        "    print(f\"   Saved to: {OUTPUT_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX2LIWDSxffE"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import yfinance as yf\n",
        "\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "print(\"Imports successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub5qBWihWjnJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# RAW DATA -> TRADING MODEL, BUILDS the Knowledge Graph\n",
        "\n",
        "!pip install torch_geometric\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import yfinance as yf\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import yfinance as yf\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "print(\"üîç Searching for data...\")\n",
        "\n",
        "possible_files = [\n",
        "    \"NVDA_Sept_2022_to_Sept_2025_News_COMPLETE.csv\",\n",
        "    \"NVDA_Sept_2022_to_Sept_2025_News.csv\",\n",
        "    \"NVDA_3Year_Processed_Data.csv\"\n",
        "]\n",
        "\n",
        "found_file = None\n",
        "for f in possible_files:\n",
        "    if os.path.exists(f):\n",
        "        found_file = f\n",
        "        break\n",
        "\n",
        "if not found_file:\n",
        "    raise FileNotFoundError(\" No CSV found! Please run your 'Fetch' code again to generate the news file.\")\n",
        "\n",
        "print(f\"Found File: {found_file}\")\n",
        "df = pd.read_csv(found_file)\n",
        "\n",
        "if 'Sentiment_Score' not in df.columns:\n",
        "    print(\" Missing AI Columns. Generating them NOW (Fast Mode)...\")\n",
        "\n",
        "    def fast_sentiment(text):\n",
        "        text = str(text).lower()\n",
        "        if any(w in text for w in ['soar', 'record', 'jump', 'beat', 'bull', 'buy', 'partner']): return 0.9\n",
        "        if any(w in text for w in ['drop', 'miss', 'fail', 'ban', 'sanction', 'sell', 'lawsuit']): return -0.9\n",
        "        return 0.1\n",
        "\n",
        "    df['Sentiment_Score'] = df['Headline'].apply(fast_sentiment)\n",
        "\n",
        "    def fast_relation(text):\n",
        "        text = str(text).lower()\n",
        "        if 'amd' in text or 'intel' in text: return 'Competitor'\n",
        "        if 'tsmc' in text or 'supply' in text: return 'Supplier'\n",
        "        if 'openai' in text or 'microsoft' in text: return 'Partner'\n",
        "        return 'General_Market'\n",
        "\n",
        "    df['Relation_Type'] = df['Headline'].apply(fast_relation)\n",
        "    print(\" AI Columns Generated.\")\n",
        "\n",
        "df.rename(columns={'Sentiment_Score': 'Weight', 'Relation_Type': 'Target'}, inplace=True)\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.sort_values('Date')\n",
        "\n",
        "print(f\"üï∏Ô∏è Building Dynamic Graph (RAM)...\")\n",
        "temporal_graphs = {}\n",
        "unique_dates = sorted(df['Date'].unique())\n",
        "DECAY_RATE = 0.95\n",
        "\n",
        "for current_date in tqdm(unique_dates, desc=\"Processing\"):\n",
        "\n",
        "    cutoff = current_date - pd.Timedelta(days=60)\n",
        "    history = df[(df['Date'] <= current_date) & (df['Date'] > cutoff)].copy()\n",
        "\n",
        "    history['Days_Old'] = (current_date - history['Date']).dt.days\n",
        "    history['Decayed_Weight'] = history['Weight'] * (DECAY_RATE ** history['Days_Old'])\n",
        "\n",
        "    G = nx.MultiDiGraph()\n",
        "    for _, row in history.iterrows():\n",
        "        if abs(row['Decayed_Weight']) > 0.05:\n",
        "            G.add_edge(\"Nvidia\", row['Target'], weight=row['Decayed_Weight'])\n",
        "\n",
        "    temporal_graphs[current_date.strftime('%Y-%m-%d')] = G\n",
        "\n",
        "print(\"üìâ Fetching Prices...\")\n",
        "graph_dates = sorted(list(temporal_graphs.keys()))\n",
        "start, end = graph_dates[0], graph_dates[-1]\n",
        "fetch_start = (pd.to_datetime(start) - pd.Timedelta(days=60)).strftime('%Y-%m-%d')\n",
        "\n",
        "df_price = yf.download(\"NVDA\", start=fetch_start, end=end, progress=False)\n",
        "if isinstance(df_price.columns, pd.MultiIndex): df_price.columns = [c[0] for c in df_price.columns]\n",
        "df_price.reset_index(inplace=True)\n",
        "df_price['Date'] = df_price['Date'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "df_price['SMA'] = df_price['Close'].rolling(14).mean()\n",
        "df_price['Return'] = df_price['Close'].pct_change()\n",
        "df_price['Vol'] = df_price['Close'].rolling(14).std()\n",
        "df_price.dropna(inplace=True)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "kpi_cols = ['Close', 'SMA', 'Return', 'Vol']\n",
        "scaled_data = scaler.fit_transform(df_price[kpi_cols].values)\n",
        "date_to_kpi = {r['Date']: scaled_data[i] for i, r in df_price.iterrows()}\n",
        "\n",
        "all_nodes = set()\n",
        "for g in temporal_graphs.values(): all_nodes.update(g.nodes())\n",
        "node_to_idx = {n: i for i, n in enumerate(sorted(list(all_nodes)))}\n",
        "NUM_NODES = len(node_to_idx)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_pyg(date_str):\n",
        "    g = temporal_graphs.get(date_str)\n",
        "    if not g or g.number_of_edges() == 0: return None\n",
        "    edges = [[node_to_idx[u], node_to_idx[v]] for u,v in g.edges()]\n",
        "    weights = [d['weight'] for u,v,d in g.edges(data=True)]\n",
        "    return Data(x=torch.eye(NUM_NODES), edge_index=torch.tensor(edges).t().contiguous(), edge_attr=torch.tensor(weights).float())\n",
        "\n",
        "seqs, targets, target_dates = [], [], []\n",
        "valid_dates = sorted([d for d in graph_dates if d in date_to_kpi])\n",
        "SEQ_LEN = 5\n",
        "\n",
        "for i in range(len(valid_dates) - SEQ_LEN):\n",
        "    dates_win = valid_dates[i:i+SEQ_LEN]\n",
        "    target_d = valid_dates[i+SEQ_LEN]\n",
        "    gs = [get_pyg(d) for d in dates_win]\n",
        "    if any(g is None for g in gs): continue\n",
        "    seqs.append((gs, [date_to_kpi[d] for d in dates_win]))\n",
        "    targets.append(date_to_kpi[target_d][0])\n",
        "    target_dates.append(target_d)\n",
        "\n",
        "train_size = len(seqs) - 60\n",
        "train_data = seqs[:train_size]\n",
        "test_data = seqs[train_size:]\n",
        "train_y = targets[:train_size]\n",
        "test_y = targets[train_size:]\n",
        "test_dates = target_dates[train_size:]\n",
        "\n",
        "class FastTrader(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.gnn = GCNConv(NUM_NODES, 32)\n",
        "        self.lstm = nn.LSTM(32+4, 64, batch_first=True)\n",
        "        self.head = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, gs, kpis):\n",
        "        embeds = []\n",
        "        for g in gs:\n",
        "            x, idx, w = g.x.to(device), g.edge_index.to(device), g.edge_attr.to(device)\n",
        "            x = torch.tanh(self.gnn(x, idx, w))\n",
        "            embeds.append(global_mean_pool(x, torch.zeros(x.size(0), dtype=torch.long).to(device)))\n",
        "        return self.head(self.lstm(torch.cat((torch.stack(embeds, 1), kpis), 2))[0][:, -1, :])\n",
        "\n",
        "print(f\"\\nüöÄ Training on {device}...\")\n",
        "model = FastTrader().to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "for ep in range(30):\n",
        "    model.train()\n",
        "    tot_loss = 0\n",
        "    for i in range(len(train_data)):\n",
        "        gs, kpis = train_data[i]\n",
        "        tgt = torch.tensor([[train_y[i]]]).float().to(device)\n",
        "        kpi_in = torch.tensor(kpis).float().unsqueeze(0).to(device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss = loss_fn(model(gs, kpi_in), tgt)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        tot_loss += loss.item()\n",
        "    if ep % 5 == 0: print(f\"Epoch {ep} | Loss: {tot_loss/len(train_data):.5f}\")\n",
        "\n",
        "model.eval()\n",
        "preds, acts = [], []\n",
        "for i in range(len(test_data)):\n",
        "    gs, kpis = test_data[i]\n",
        "    pred = model(gs, torch.tensor(kpis).float().unsqueeze(0).to(device)).item()\n",
        "\n",
        "    d = np.zeros((1, 4))\n",
        "    d[0,0] = pred\n",
        "    preds.append(scaler.inverse_transform(d)[0,0])\n",
        "    d[0,0] = test_y[i]\n",
        "    acts.append(scaler.inverse_transform(d)[0,0])\n",
        "\n",
        "y_act, y_pred = np.array(acts), np.array(preds)\n",
        "acc = accuracy_score((np.diff(y_act) > 0), ((y_pred[1:] - y_act[:-1]) > 0)) * 100\n",
        "print(f\"\\n FINAL ACCURACY: {acc:.2f}%\")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(test_dates, acts, label='Actual')\n",
        "plt.plot(test_dates, preds, label='AI Prediction', linestyle='--')\n",
        "plt.title(f\"Fixer Script Result (Acc: {acc:.2f}%)\"); plt.legend(); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMM5lO5mz8EF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import pickle\n",
        "\n",
        "input_file = 'NVDA_3Year_Processed_Data.csv'\n",
        "output_file = 'NVDA_Dynamic_Graph.pkl'\n",
        "\n",
        "relation_types = ['Ego', 'Competitor', 'Regulatory', 'Partner', 'Supplier', 'Customer', 'Neutral']\n",
        "rel_to_idx = {r: i for i, r in enumerate(relation_types)}\n",
        "num_relations = len(relation_types)\n",
        "\n",
        "def create_knowledge_graph(csv_path, pkl_path):\n",
        "    print(f\"Loading data from {csv_path}...\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    grouped = df.groupby('Date')\n",
        "\n",
        "    dataset = []\n",
        "\n",
        "    print(\"Processing graphs...\")\n",
        "    for date, group in grouped:\n",
        "\n",
        "        num_news = len(group)\n",
        "        num_nodes = 1 + num_news\n",
        "\n",
        "        x = torch.zeros((num_nodes, num_relations + 1), dtype=torch.float)\n",
        "\n",
        "        x[0, rel_to_idx['Ego']] = 1.0\n",
        "        x[0, -1] = 0.0\n",
        "\n",
        "        for i, (_, row) in enumerate(group.iterrows()):\n",
        "            node_idx = i + 1\n",
        "            rel_type = row['Relation_Type']\n",
        "            sentiment = row['Sentiment_Score']\n",
        "\n",
        "            if rel_type in rel_to_idx:\n",
        "                x[node_idx, rel_to_idx[rel_type]] = 1.0\n",
        "\n",
        "            x[node_idx, -1] = sentiment\n",
        "\n",
        "        sources = torch.arange(1, num_nodes, dtype=torch.long)\n",
        "        targets = torch.zeros(num_news, dtype=torch.long)\n",
        "\n",
        "        edge_index = torch.stack([sources, targets], dim=0)\n",
        "\n",
        "        data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "        data.date = date\n",
        "        data.num_nodes = num_nodes\n",
        "\n",
        "        dataset.append(data)\n",
        "\n",
        "    print(f\"Saving {len(dataset)} graphs to {pkl_path}...\")\n",
        "    with open(pkl_path, 'wb') as f:\n",
        "        pickle.dump(dataset, f)\n",
        "\n",
        "    print(\"Done!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_knowledge_graph(input_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBev4f2nzrLM"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "with open('NVDA_Dynamic_Graph.pkl', 'rb') as f:\n",
        "    graph_dicts = pickle.load(f)\n",
        "\n",
        "dataset = []\n",
        "for g in graph_dicts:\n",
        "    x = torch.tensor(g['x'], dtype=torch.float)\n",
        "    edge_index = torch.tensor(g['edge_index'], dtype=torch.long)\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "    data.date = g['date']\n",
        "\n",
        "    dataset.append(data)\n",
        "\n",
        "print(f\"Successfully loaded {len(dataset)} daily graph snapshots.\")\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgvFrNhX1MJ9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "import pickle\n",
        "\n",
        "\n",
        "# Define the GCN Model\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, hidden_channels, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
        "\n",
        "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "print(\"Loading graph data...\")\n",
        "with open('NVDA_Dynamic_Graph.pkl', 'rb') as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "sample_graph = dataset[0]\n",
        "num_node_features = sample_graph.x.shape[1]\n",
        "\n",
        "print(f\"Loaded {len(dataset)} graphs.\")\n",
        "print(f\"Feature size per node: {num_node_features}\")\n",
        "\n",
        "HIDDEN_CHANNELS = 16\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "model = GCN(num_node_features, HIDDEN_CHANNELS, NUM_CLASSES)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "sample_graph = sample_graph.to(device)\n",
        "\n",
        "print(\"\\nModel Architecture:\")\n",
        "print(model)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model(sample_graph)\n",
        "\n",
        "print(\"\\nOutput shape from first graph:\", out.shape)\n",
        "print(\"First 5 node predictions (Log Softmax):\\n\", out[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZnDRuEnPei6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# GCN-LSTM: YFINANCE + STRICT DATA\n",
        "\n",
        "try:\n",
        "    import torch_geometric\n",
        "except ImportError:\n",
        "    # !pip install torch_geometric\n",
        "    import torch_geometric\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import random\n",
        "import os\n",
        "\n",
        "TICKER = \"NVDA\"\n",
        "\n",
        "START_DATE = \"2022-09-01\"\n",
        "END_DATE = \"2025-09-02\"\n",
        "TRAIN_CUTOFF = \"2025-06-15\"\n",
        "\n",
        "SEED = 42\n",
        "SEQ_LEN = 5\n",
        "EPOCHS = 50\n",
        "HIDDEN_DIM = 128\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üöÄ Training on: {device}\")\n",
        "\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Loading Knowledge Graph...\")\n",
        "try:\n",
        "    with open('NVDA_Dynamic_Graph.pkl', 'rb') as f:\n",
        "        raw_data = pickle.load(f)\n",
        "except FileNotFoundError:\n",
        "    print(\" Error: 'NVDA_Dynamic_Graph.pkl' not found.\")\n",
        "    exit()\n",
        "\n",
        "graph_map = {}\n",
        "for item in raw_data:\n",
        "    x = torch.tensor(item['x'], dtype=torch.float)\n",
        "    edge_index = torch.tensor(item['edge_index'], dtype=torch.long)\n",
        "    data = Data(x=x, edge_index=edge_index)\n",
        "    d_str = pd.to_datetime(item['date']).strftime('%Y-%m-%d')\n",
        "    graph_map[d_str] = data\n",
        "\n",
        "print(f\" Loaded {len(graph_map)} graph snapshots.\")\n",
        "\n",
        "\n",
        "print(f\"\\n2Ô∏è Fetching Stock Data via yfinance ({START_DATE} -> {END_DATE})...\")\n",
        "\n",
        "df = yf.download(TICKER, start=START_DATE, end=END_DATE, progress=False)\n",
        "\n",
        "if isinstance(df.columns, pd.MultiIndex):\n",
        "    df.columns = [c[0] for c in df.columns]\n",
        "\n",
        "df.reset_index(inplace=True)\n",
        "df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "print(f\" Fetched {len(df)} rows.\")\n",
        "\n",
        "df['SMA_14'] = df['Close'].rolling(window=14).mean()\n",
        "df['MACD'] = df['Close'].ewm(span=12).mean() - df['Close'].ewm(span=26).mean()\n",
        "df['Return'] = df['Close'].pct_change()\n",
        "\n",
        "delta = df['Close'].diff()\n",
        "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "rs = gain / loss\n",
        "df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "\n",
        "df['Target_Price'] = df['Close'].shift(-1)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "print(f\"\\n3Ô∏è Splitting Data (Cutoff: {TRAIN_CUTOFF})...\")\n",
        "\n",
        "train_df = df[df['Date'] <= TRAIN_CUTOFF].copy()\n",
        "test_df  = df[df['Date'] > TRAIN_CUTOFF].copy()\n",
        "\n",
        "print(f\"   Train Rows: {len(train_df)}\")\n",
        "print(f\"   Test Rows:  {len(test_df)}\")\n",
        "\n",
        "KPI_COLS = ['Close', 'RSI', 'MACD', 'SMA_14', 'Return', 'Target_Price']\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train_df[KPI_COLS])\n",
        "\n",
        "train_df[KPI_COLS] = scaler.transform(train_df[KPI_COLS])\n",
        "test_df[KPI_COLS]  = scaler.transform(test_df[KPI_COLS])\n",
        "\n",
        "full_df_scaled = pd.concat([train_df, test_df])\n",
        "date_to_features = {row['Date']: row[KPI_COLS[:-1]].values.astype(float) for i, row in full_df_scaled.iterrows()}\n",
        "date_to_target   = {row['Date']: row['Target_Price'] for i, row in full_df_scaled.iterrows()}\n",
        "\n",
        "\n",
        "sequences = []\n",
        "targets = []\n",
        "target_dates = []\n",
        "\n",
        "valid_stock_dates = set(date_to_features.keys())\n",
        "valid_graph_dates = set(graph_map.keys())\n",
        "common_dates = sorted(list(valid_stock_dates.intersection(valid_graph_dates)))\n",
        "\n",
        "print(f\"   Aligned {len(common_dates)} common days.\")\n",
        "\n",
        "for i in range(len(common_dates) - SEQ_LEN):\n",
        "    seq_dates = common_dates[i : i+SEQ_LEN]\n",
        "    target_date = seq_dates[-1]\n",
        "\n",
        "    if target_date not in date_to_target: continue\n",
        "\n",
        "    seq_graphs = [graph_map[d] for d in seq_dates]\n",
        "    seq_kpis = [date_to_features[d] for d in seq_dates]\n",
        "    target_val = date_to_target[target_date]\n",
        "\n",
        "    sequences.append((seq_graphs, seq_kpis))\n",
        "    targets.append(target_val)\n",
        "    target_dates.append(target_date)\n",
        "\n",
        "train_seqs, train_targets = [], []\n",
        "test_seqs, test_targets, test_dates_list = [], [], []\n",
        "\n",
        "for i, date in enumerate(target_dates):\n",
        "    if date <= TRAIN_CUTOFF:\n",
        "        train_seqs.append(sequences[i])\n",
        "        train_targets.append(targets[i])\n",
        "    else:\n",
        "        test_seqs.append(sequences[i])\n",
        "        test_targets.append(targets[i])\n",
        "        test_dates_list.append(date)\n",
        "\n",
        "print(f\"   Final Train Seqs: {len(train_seqs)}\")\n",
        "print(f\"   Final Test Seqs:  {len(test_seqs)}\")\n",
        "\n",
        "# MODEL (GCN + LSTM)\n",
        "class IntegratedTrader(nn.Module):\n",
        "    def __init__(self, node_feat_dim, kpi_dim, gnn_out=32, lstm_hidden=128):\n",
        "        super(IntegratedTrader, self).__init__()\n",
        "        self.gnn1 = GCNConv(node_feat_dim, 64)\n",
        "        self.gnn2 = GCNConv(64, gnn_out)\n",
        "        self.lstm = nn.LSTM(gnn_out + kpi_dim, lstm_hidden, batch_first=True, dropout=0.2)\n",
        "        self.head = nn.Linear(lstm_hidden, 1)\n",
        "\n",
        "    def forward(self, graph_list, kpi_tensor):\n",
        "        sentiment_vecs = []\n",
        "        for g in graph_list:\n",
        "            x, edge_index = g.x.to(device), g.edge_index.to(device)\n",
        "            x = F.relu(self.gnn1(x, edge_index))\n",
        "            x = self.gnn2(x, edge_index)\n",
        "            batch = torch.zeros(x.size(0), dtype=torch.long).to(device)\n",
        "            day_vec = global_mean_pool(x, batch)\n",
        "            sentiment_vecs.append(day_vec)\n",
        "\n",
        "        sentiment_seq = torch.stack(sentiment_vecs, dim=1)\n",
        "        fusion = torch.cat((sentiment_seq, kpi_tensor), dim=2)\n",
        "        lstm_out, _ = self.lstm(fusion)\n",
        "        return self.head(lstm_out[:, -1, :])\n",
        "\n",
        "class AsymmetricDirectionalLoss(nn.Module):\n",
        "    def __init__(self, penalty=2.0, bullish_weight=4.0):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.penalty = penalty\n",
        "        self.bullish_weight = bullish_weight\n",
        "\n",
        "    def forward(self, pred, target, prev_price):\n",
        "        loss_mse = self.mse(pred, target)\n",
        "        true_diff = target - prev_price\n",
        "        pred_diff = pred - prev_price\n",
        "        wrong_dir = (true_diff * pred_diff) < 0\n",
        "        missed_up = (true_diff > 0) & wrong_dir\n",
        "        dir_cost = torch.abs(true_diff - pred_diff)\n",
        "        weighted_cost = torch.where(missed_up, dir_cost * self.bullish_weight, dir_cost)\n",
        "        return loss_mse + (self.penalty * torch.mean(wrong_dir.float() * weighted_cost))\n",
        "\n",
        "if not train_seqs:\n",
        "    print(\" Error: No training sequences found.\")\n",
        "    exit()\n",
        "\n",
        "sample_graph = sequences[0][0][0]\n",
        "NODE_DIM = sample_graph.x.shape[1]\n",
        "KPI_DIM = len(KPI_COLS) - 1\n",
        "\n",
        "model = IntegratedTrader(NODE_DIM, KPI_DIM, lstm_hidden=HIDDEN_DIM).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0008)\n",
        "criterion = AsymmetricDirectionalLoss(penalty=2.0, bullish_weight=4.0)\n",
        "\n",
        "print(f\"\\nüöÄ Starting Training ({EPOCHS} Epochs)...\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i in range(len(train_seqs)):\n",
        "        seq_graphs, seq_kpis = train_seqs[i]\n",
        "        target = torch.tensor([[train_targets[i]]], dtype=torch.float).to(device)\n",
        "        kpi_input = torch.tensor(np.array(seq_kpis), dtype=torch.float).unsqueeze(0).to(device)\n",
        "        prev_price = kpi_input[:, -1, 0].unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(seq_graphs, kpi_input)\n",
        "        loss = criterion(pred, target, prev_price)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f\"   Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss/len(train_seqs):.5f}\")\n",
        "\n",
        "MODEL_FILE = \"NVDA_Sniper_Model.pth\"\n",
        "torch.save(model.state_dict(), MODEL_FILE)\n",
        "print(f\"\\n Model saved to: {MODEL_FILE}\")\n",
        "print(\"You can now run the Live Agent script!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\" FINAL EVALUATION (16 June 2025 -> 1 Sept 2025)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "model.eval()\n",
        "preds, actuals = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(len(test_seqs)):\n",
        "        seq_graphs, seq_kpis = test_seqs[i]\n",
        "        kpi_input = torch.tensor(np.array(seq_kpis), dtype=torch.float).unsqueeze(0).to(device)\n",
        "        pred = model(seq_graphs, kpi_input).item()\n",
        "        preds.append(pred)\n",
        "        actuals.append(test_targets[i])\n",
        "\n",
        "def unscale(vals):\n",
        "    dummy = np.zeros((len(vals), len(KPI_COLS)))\n",
        "    dummy[:, -1] = vals\n",
        "    return scaler.inverse_transform(dummy)[:, -1]\n",
        "\n",
        "final_preds = unscale(preds)\n",
        "final_acts = unscale(actuals)\n",
        "\n",
        "prev_prices_scaled = [seq[1][-1][0] for seq in test_seqs]\n",
        "dummy_prev = np.zeros((len(prev_prices_scaled), len(KPI_COLS)))\n",
        "dummy_prev[:, 0] = prev_prices_scaled\n",
        "prev_prices_real = scaler.inverse_transform(dummy_prev)[:, 0]\n",
        "\n",
        "mae = mean_absolute_error(final_acts, final_preds)\n",
        "true_dir = (final_acts > prev_prices_real).astype(int)\n",
        "pred_dir = (final_preds > prev_prices_real).astype(int)\n",
        "\n",
        "acc = accuracy_score(true_dir, pred_dir)\n",
        "prec = precision_score(true_dir, pred_dir, zero_division=0)\n",
        "rec = recall_score(true_dir, pred_dir, zero_division=0)\n",
        "f1 = f1_score(true_dir, pred_dir, zero_division=0)\n",
        "\n",
        "print(f\" MAE: ${mae:.2f}\")\n",
        "print(f\" Accuracy:  {acc:.2%}\")\n",
        "print(f\" Precision: {prec:.2%}\")\n",
        "print(f\" Recall:    {rec:.2%}\")\n",
        "print(f\" F1 Score:  {f1:.4f}\")\n",
        "\n",
        "\n",
        "plot_dates = [pd.to_datetime(d) for d in test_dates_list]\n",
        "\n",
        "correct_indices = [i for i in range(len(true_dir)) if true_dir[i] == pred_dir[i]]\n",
        "incorrect_indices = [i for i in range(len(true_dir)) if true_dir[i] != pred_dir[i]]\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(plot_dates, final_acts, label='Actual Price', color='black', alpha=0.6)\n",
        "plt.plot(plot_dates, final_preds, label='AI Prediction', color='blue', linestyle='--')\n",
        "\n",
        "plt.scatter([plot_dates[i] for i in correct_indices], [final_acts[i] for i in correct_indices],\n",
        "            color='green', marker='^', s=50, label='Correct')\n",
        "plt.scatter([plot_dates[i] for i in incorrect_indices], [final_acts[i] for i in incorrect_indices],\n",
        "            color='red', marker='v', s=50, label='Wrong')\n",
        "\n",
        "plt.title(f'NVDA Prediction (YFinance) | Acc: {acc:.2%}', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
        "plt.gcf().autofmt_xdate()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBQSM-F7YBO7"
      },
      "outputs": [],
      "source": [
        "# Run this NOW to save the model currently in your RAM\n",
        "torch.save(model.state_dict(), \"NVDA_Sniper_Model.pth\")\n",
        "print(\" Saved! You can now run the Agent.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQkfI4anYl7I"
      },
      "outputs": [],
      "source": [
        "\n",
        "# NVDA TIME-TRAVEL TRADING AGENT\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "TICKER = \"NVDA\"\n",
        "MODEL_FILE = \"NVDA_Sniper_Model.pth\"\n",
        "SEQ_LEN = 5\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_trading_day_before(target_date_str):\n",
        "    \"\"\"Finds the trading day immediately BEFORE the target date.\"\"\"\n",
        "    target_dt = datetime.strptime(target_date_str, '%Y-%m-%d')\n",
        "\n",
        "    prev_dt = target_dt - timedelta(days=1)\n",
        "    while prev_dt.weekday() > 4:\n",
        "        prev_dt -= timedelta(days=1)\n",
        "\n",
        "    return prev_dt.strftime('%Y-%m-%d')\n",
        "\n",
        "def get_historical_stock_data(end_date_str):\n",
        "    \"\"\"\n",
        "    Fetches stock data ending EXACTLY on end_date_str.\n",
        "    We need the last SEQ_LEN days leading up to this date.\n",
        "    \"\"\"\n",
        "    print(f\" Fetching Stock Data up to {end_date_str}...\")\n",
        "\n",
        "    end_dt_obj = datetime.strptime(end_date_str, '%Y-%m-%d') + timedelta(days=1)\n",
        "    query_end = end_dt_obj.strftime('%Y-%m-%d')\n",
        "    start_dt_obj = end_dt_obj - timedelta(days=90)\n",
        "    query_start = start_dt_obj.strftime('%Y-%m-%d')\n",
        "\n",
        "    df = yf.download(TICKER, start=query_start, end=query_end, progress=False)\n",
        "\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [c[0] for c in df.columns]\n",
        "\n",
        "    df.reset_index(inplace=True)\n",
        "    df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "    df['SMA_14'] = df['Close'].rolling(window=14).mean()\n",
        "    df['MACD'] = df['Close'].ewm(span=12).mean() - df['Close'].ewm(span=26).mean()\n",
        "    df['Return'] = df['Close'].pct_change()\n",
        "\n",
        "    delta = df['Close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "    rs = gain / loss\n",
        "    df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    df_filtered = df[df['Date'] <= end_date_str].copy()\n",
        "\n",
        "    return df_filtered.tail(SEQ_LEN)\n",
        "\n",
        "def update_knowledge_graph(date_str):\n",
        "    \"\"\"\n",
        "    Simulates fetching news for 'date_str' and updating the graph.\n",
        "    In a real app, this would scrape news and decay old weights.\n",
        "    \"\"\"\n",
        "    print(f\"üï∏Ô∏è Updating Knowledge Graph for {date_str}...\")\n",
        "\n",
        "    num_nodes = 50\n",
        "    x = torch.eye(num_nodes)\n",
        "    edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long)\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index)\n",
        "\n",
        "class IntegratedTrader(nn.Module):\n",
        "    def __init__(self, node_feat_dim, kpi_dim, gnn_out=32, lstm_hidden=128):\n",
        "        super(IntegratedTrader, self).__init__()\n",
        "        self.gnn1 = GCNConv(node_feat_dim, 64)\n",
        "        self.gnn2 = GCNConv(64, gnn_out)\n",
        "        self.lstm = nn.LSTM(gnn_out + kpi_dim, lstm_hidden, batch_first=True, dropout=0.2)\n",
        "        self.head = nn.Linear(lstm_hidden, 1)\n",
        "\n",
        "    def forward(self, graph_list, kpi_tensor):\n",
        "        sentiment_vecs = []\n",
        "        for g in graph_list:\n",
        "            x, edge_index = g.x.to(DEVICE), g.edge_index.to(DEVICE)\n",
        "            x = F.relu(self.gnn1(x, edge_index))\n",
        "            x = self.gnn2(x, edge_index)\n",
        "            batch = torch.zeros(x.size(0), dtype=torch.long).to(DEVICE)\n",
        "            day_vec = global_mean_pool(x, batch)\n",
        "            sentiment_vecs.append(day_vec)\n",
        "\n",
        "        sentiment_seq = torch.stack(sentiment_vecs, dim=1)\n",
        "        fusion = torch.cat((sentiment_seq, kpi_tensor), dim=2)\n",
        "        lstm_out, _ = self.lstm(fusion)\n",
        "        return self.head(lstm_out[:, -1, :])\n",
        "\n",
        "def run_prediction(target_date_str):\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\" AGENT ACTIVATED | TARGET: {target_date_str}\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    decision_date_str = get_trading_day_before(target_date_str)\n",
        "    print(f\" Decision Context Date: {decision_date_str}\")\n",
        "\n",
        "    df_seq = get_historical_stock_data(decision_date_str)\n",
        "\n",
        "    if len(df_seq) < SEQ_LEN:\n",
        "        print(f\" Error: Not enough data found ending on {decision_date_str}\")\n",
        "        print(\"   (Check if the date is a valid trading day or too far in the past/future)\")\n",
        "        return\n",
        "\n",
        "    KPI_COLS = ['Close', 'RSI', 'MACD', 'SMA_14', 'Return']\n",
        "    scaler = MinMaxScaler()\n",
        "    df_scaled = df_seq.copy()\n",
        "\n",
        "    df_scaled[KPI_COLS] = scaler.fit_transform(df_seq[KPI_COLS])\n",
        "\n",
        "    seq_kpis = df_scaled[KPI_COLS].values\n",
        "    kpi_tensor = torch.tensor(seq_kpis, dtype=torch.float).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    graph_snapshot = update_knowledge_graph(decision_date_str)\n",
        "    graph_seq = [graph_snapshot] * SEQ_LEN\n",
        "\n",
        "    NODE_DIM = 50\n",
        "    KPI_DIM = len(KPI_COLS)\n",
        "\n",
        "    model = IntegratedTrader(node_feat_dim=NODE_DIM, kpi_dim=KPI_DIM, lstm_hidden=128).to(DEVICE)\n",
        "\n",
        "    if os.path.exists(MODEL_FILE):\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(MODEL_FILE, map_location=DEVICE))\n",
        "            print(f\" Loaded Trained Model: {MODEL_FILE}\")\n",
        "            model.eval()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred_scaled = model(graph_seq, kpi_tensor).item()\n",
        "        except Exception as e:\n",
        "            print(f\" Error loading model: {e}\")\n",
        "            pred_scaled = 0.5\n",
        "    else:\n",
        "        print(\"\\n WARNING: 'NVDA_Sniper_Model.pth' NOT FOUND.\")\n",
        "        print(\"   >> Running with UNTRAINED (Random) weights to demonstrate logic.\")\n",
        "        print(\"   >> The prediction below is technically meaningless until you train/save the model.\")\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred_scaled = model(graph_seq, kpi_tensor).item()\n",
        "\n",
        "    dummy = np.zeros((1, len(KPI_COLS)))\n",
        "    dummy[0, 0] = pred_scaled\n",
        "    price_pred = scaler.inverse_transform(dummy)[0, 0]\n",
        "\n",
        "    last_close = df_seq['Close'].iloc[-1]\n",
        "\n",
        "    print(\"\\n\" + \"-\"*30)\n",
        "    print(f\" Close on {decision_date_str}:  ${last_close:.2f}\")\n",
        "    print(f\" Forecast for {target_date_str}:  ${price_pred:.2f}\")\n",
        "\n",
        "    change_pct = ((price_pred - last_close) / last_close) * 100\n",
        "    print(f\"üìä Predicted Move: {change_pct:.2f}%\")\n",
        "\n",
        "    if price_pred > last_close * 1.005:\n",
        "        print(\" SIGNAL: BUY (Bullish)\")\n",
        "    elif price_pred < last_close * 0.995:\n",
        "        print(\" SIGNAL: SELL (Bearish)\")\n",
        "    else:\n",
        "        print(\" SIGNAL: HOLD (Neutral)\")\n",
        "    print(\"-\"*30)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    target = \"2025-09-05\"\n",
        "    run_prediction(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-x_22bwZV6Q"
      },
      "outputs": [],
      "source": [
        "pip install streamlit plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CumEQMxxZeZ-"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from datetime import datetime, timedelta\n",
        "import plotly.graph_objects as go\n",
        "import os\n",
        "\n",
        "TICKER = \"NVDA\"\n",
        "MODEL_FILE = \"NVDA_Sniper_Model.pth\"\n",
        "SEQ_LEN = 5\n",
        "DEVICE = torch.device('cpu')\n",
        "\n",
        "class IntegratedTrader(nn.Module):\n",
        "    def __init__(self, node_feat_dim, kpi_dim, gnn_out=32, lstm_hidden=128):\n",
        "        super(IntegratedTrader, self).__init__()\n",
        "        self.gnn1 = GCNConv(node_feat_dim, 64)\n",
        "        self.gnn2 = GCNConv(64, gnn_out)\n",
        "        self.lstm = nn.LSTM(gnn_out + kpi_dim, lstm_hidden, batch_first=True, dropout=0.2)\n",
        "        self.head = nn.Linear(lstm_hidden, 1)\n",
        "\n",
        "    def forward(self, graph_list, kpi_tensor):\n",
        "        sentiment_vecs = []\n",
        "        for g in graph_list:\n",
        "            x, edge_index = g.x.to(DEVICE), g.edge_index.to(DEVICE)\n",
        "            x = F.relu(self.gnn1(x, edge_index))\n",
        "            x = self.gnn2(x, edge_index)\n",
        "            batch = torch.zeros(x.size(0), dtype=torch.long).to(DEVICE)\n",
        "            day_vec = global_mean_pool(x, batch)\n",
        "            sentiment_vecs.append(day_vec)\n",
        "\n",
        "        sentiment_seq = torch.stack(sentiment_vecs, dim=1)\n",
        "        fusion = torch.cat((sentiment_seq, kpi_tensor), dim=2)\n",
        "        lstm_out, _ = self.lstm(fusion)\n",
        "        return self.head(lstm_out[:, -1, :])\n",
        "\n",
        "def get_historical_stock_data(end_date_str):\n",
        "    \"\"\"Fetches data ending exactly on the decision day.\"\"\"\n",
        "    end_dt_obj = datetime.strptime(end_date_str, '%Y-%m-%d') + timedelta(days=1)\n",
        "    query_end = end_dt_obj.strftime('%Y-%m-%d')\n",
        "    start_dt_obj = end_dt_obj - timedelta(days=90)\n",
        "    query_start = start_dt_obj.strftime('%Y-%m-%d')\n",
        "\n",
        "    df = yf.download(TICKER, start=query_start, end=query_end, progress=False)\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [c[0] for c in df.columns]\n",
        "    df.reset_index(inplace=True)\n",
        "    df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "    # Indicators\n",
        "    df['SMA_14'] = df['Close'].rolling(window=14).mean()\n",
        "    df['MACD'] = df['Close'].ewm(span=12).mean() - df['Close'].ewm(span=26).mean()\n",
        "    df['Return'] = df['Close'].pct_change()\n",
        "    delta = df['Close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "    rs = gain / loss\n",
        "    df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    return df[df['Date'] <= end_date_str].copy()\n",
        "\n",
        "def get_trading_day_before(target_date_obj):\n",
        "    \"\"\"Finds previous trading day.\"\"\"\n",
        "    prev_dt = target_date_obj - timedelta(days=1)\n",
        "    while prev_dt.weekday() > 4:\n",
        "        prev_dt -= timedelta(days=1)\n",
        "    return prev_dt.strftime('%Y-%m-%d')\n",
        "\n",
        "\n",
        "st.set_page_config(page_title=\"NVDA Sniper Agent\", page_icon=\"üéØ\", layout=\"wide\")\n",
        "\n",
        "st.title(\"üéØ NVDA Sniper: AI Investment Agent\")\n",
        "st.markdown(\"\"\"\n",
        "**Wanna invest in NVIDIA?** Let the AI check the **Knowledge Graph** and **Price Trends** for you.\n",
        "\"\"\")\n",
        "\n",
        "st.sidebar.header(\"‚öôÔ∏è Configuration\")\n",
        "target_date_input = st.sidebar.date_input(\"Target Date for Prediction\", datetime.today() + timedelta(days=1))\n",
        "target_date_str = target_date_input.strftime('%Y-%m-%d')\n",
        "\n",
        "if st.sidebar.button(\"üöÄ Run Analysis\"):\n",
        "    with st.spinner(f\"Traveling back in time to analyze context for {target_date_str}...\"):\n",
        "\n",
        "        decision_date_str = get_trading_day_before(target_date_input)\n",
        "\n",
        "        df_seq = get_historical_stock_data(decision_date_str)\n",
        "\n",
        "        if len(df_seq) < SEQ_LEN:\n",
        "            st.error(f\"Not enough data found ending on {decision_date_str}. Try a more recent date.\")\n",
        "        else:\n",
        "\n",
        "            KPI_COLS = ['Close', 'RSI', 'MACD', 'SMA_14', 'Return']\n",
        "            scaler = MinMaxScaler()\n",
        "            df_scaled = df_seq.copy()\n",
        "            df_scaled[KPI_COLS] = scaler.fit_transform(df_seq[KPI_COLS])\n",
        "\n",
        "            seq_kpis = df_scaled[KPI_COLS].tail(SEQ_LEN).values\n",
        "            kpi_tensor = torch.tensor(seq_kpis, dtype=torch.float).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "            x = torch.eye(50)\n",
        "            edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long)\n",
        "            graph_snapshot = Data(x=x, edge_index=edge_index)\n",
        "            graph_seq = [graph_snapshot] * SEQ_LEN\n",
        "\n",
        "            model = IntegratedTrader(node_feat_dim=50, kpi_dim=len(KPI_COLS), lstm_hidden=128).to(DEVICE)\n",
        "\n",
        "            model_loaded = False\n",
        "            if os.path.exists(MODEL_FILE):\n",
        "                try:\n",
        "                    model.load_state_dict(torch.load(MODEL_FILE, map_location=DEVICE))\n",
        "                    model.eval()\n",
        "                    model_loaded = True\n",
        "                except:\n",
        "                    st.warning(\" Model file corrupted. Using Random Weights (Demo Mode).\")\n",
        "            else:\n",
        "                st.warning(\" 'NVDA_Sniper_Model.pth' not found. Using Random Weights (Demo Mode).\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred_scaled = model(graph_seq, kpi_tensor).item() if model_loaded else 0.55\n",
        "\n",
        "            dummy = np.zeros((1, len(KPI_COLS)))\n",
        "            dummy[0, 0] = pred_scaled\n",
        "            price_pred = scaler.inverse_transform(dummy)[0, 0]\n",
        "\n",
        "            last_close = df_seq['Close'].iloc[-1]\n",
        "            change_pct = ((price_pred - last_close) / last_close) * 100\n",
        "\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "            col1.metric(\"Last Close Price\", f\"${last_close:.2f}\", f\"On {decision_date_str}\")\n",
        "            col2.metric(\"AI Target Price\", f\"${price_pred:.2f}\", f\"{change_pct:.2f}%\")\n",
        "\n",
        "            if price_pred > last_close * 1.005:\n",
        "                signal = \"BUY \"\n",
        "                color = \"green\"\n",
        "            elif price_pred < last_close * 0.995:\n",
        "                signal = \"SELL \"\n",
        "                color = \"red\"\n",
        "            else:\n",
        "                signal = \"HOLD \"\n",
        "                color = \"gray\"\n",
        "\n",
        "            col3.markdown(f\"### Signal: :{color}[{signal}]\")\n",
        "\n",
        "            st.markdown(\"###  Price Movement & Prediction\")\n",
        "\n",
        "            fig = go.Figure()\n",
        "\n",
        "            plot_df = df_seq.tail(30)\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=plot_df['Date'],\n",
        "                y=plot_df['Close'],\n",
        "                mode='lines',\n",
        "                name='History',\n",
        "                line=dict(color='#00d2be', width=2)\n",
        "            ))\n",
        "\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=[target_date_str],\n",
        "                y=[price_pred],\n",
        "                mode='markers',\n",
        "                name='AI Prediction',\n",
        "                marker=dict(color='red' if 'SELL' in signal else 'green', size=14, symbol='star')\n",
        "            ))\n",
        "\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=[plot_df['Date'].iloc[-1], target_date_str],\n",
        "                y=[plot_df['Close'].iloc[-1], price_pred],\n",
        "                mode='lines',\n",
        "                line=dict(color='gray', dash='dot'),\n",
        "                showlegend=False\n",
        "            ))\n",
        "\n",
        "            fig.update_layout(template=\"plotly_dark\", height=400)\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "            st.info(f\"‚ÑπÔ∏è **Analysis:** The AI analyzed market structure up to **{decision_date_str}**. \"\n",
        "                    f\"It detected a **{change_pct:.2f}%** potential move based on technicals \"\n",
        "                    \"and (simulated) knowledge graph sentiment.\")\n",
        "\n",
        "else:\n",
        "    st.info(\" Select a date in the sidebar and click 'Run Analysis' to start.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj0rQUJ3clFE"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from datetime import datetime, timedelta\n",
        "import plotly.graph_objects as go\n",
        "import os\n",
        "\n",
        "TICKER = \"NVDA\"\n",
        "MODEL_FILE = \"NVDA_Sniper_Model.pth\"\n",
        "SEQ_LEN = 5\n",
        "DEVICE = torch.device('cpu')\n",
        "\n",
        "class IntegratedTrader(nn.Module):\n",
        "    def __init__(self, node_feat_dim, kpi_dim, gnn_out=32, lstm_hidden=128):\n",
        "        super(IntegratedTrader, self).__init__()\n",
        "        self.gnn1 = GCNConv(node_feat_dim, 64)\n",
        "        self.gnn2 = GCNConv(64, gnn_out)\n",
        "        self.lstm = nn.LSTM(gnn_out + kpi_dim, lstm_hidden, batch_first=True, dropout=0.2)\n",
        "        self.head = nn.Linear(lstm_hidden, 1)\n",
        "\n",
        "    def forward(self, graph_list, kpi_tensor):\n",
        "        sentiment_vecs = []\n",
        "        for g in graph_list:\n",
        "            x, edge_index = g.x.to(DEVICE), g.edge_index.to(DEVICE)\n",
        "            x = F.relu(self.gnn1(x, edge_index))\n",
        "            x = self.gnn2(x, edge_index)\n",
        "            batch = torch.zeros(x.size(0), dtype=torch.long).to(DEVICE)\n",
        "            day_vec = global_mean_pool(x, batch)\n",
        "            sentiment_vecs.append(day_vec)\n",
        "\n",
        "        sentiment_seq = torch.stack(sentiment_vecs, dim=1)\n",
        "        fusion = torch.cat((sentiment_seq, kpi_tensor), dim=2)\n",
        "        lstm_out, _ = self.lstm(fusion)\n",
        "        return self.head(lstm_out[:, -1, :])\n",
        "\n",
        "def get_historical_stock_data(end_date_str):\n",
        "    end_dt_obj = datetime.strptime(end_date_str, '%Y-%m-%d') + timedelta(days=1)\n",
        "    query_end = end_dt_obj.strftime('%Y-%m-%d')\n",
        "    start_dt_obj = end_dt_obj - timedelta(days=90)\n",
        "    query_start = start_dt_obj.strftime('%Y-%m-%d')\n",
        "\n",
        "    df = yf.download(TICKER, start=query_start, end=query_end, progress=False)\n",
        "    if isinstance(df.columns, pd.MultiIndex): df.columns = [c[0] for c in df.columns]\n",
        "    df.reset_index(inplace=True)\n",
        "    df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "    df['SMA_14'] = df['Close'].rolling(window=14).mean()\n",
        "    df['MACD'] = df['Close'].ewm(span=12).mean() - df['Close'].ewm(span=26).mean()\n",
        "    df['Return'] = df['Close'].pct_change()\n",
        "    delta = df['Close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "    rs = gain / loss\n",
        "    df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    return df[df['Date'] <= end_date_str].copy()\n",
        "\n",
        "def get_trading_day_before(target_date_obj):\n",
        "    prev_dt = target_date_obj - timedelta(days=1)\n",
        "    while prev_dt.weekday() > 4:\n",
        "        prev_dt -= timedelta(days=1)\n",
        "    return prev_dt.strftime('%Y-%m-%d')\n",
        "\n",
        "st.set_page_config(page_title=\"NVDA Sniper Agent\", page_icon=\"\", layout=\"wide\")\n",
        "st.title(\" NVDA Sniper: AI Investment Agent\")\n",
        "st.markdown(\"**Wanna invest in NVIDIA?** Let the AI check the **Knowledge Graph** and **Price Trends**.\")\n",
        "\n",
        "st.sidebar.header(\"‚öôÔ∏è Configuration\")\n",
        "target_date_input = st.sidebar.date_input(\"Target Date for Prediction\", datetime.today() + timedelta(days=1))\n",
        "target_date_str = target_date_input.strftime('%Y-%m-%d')\n",
        "\n",
        "if st.sidebar.button(\" Run Analysis\"):\n",
        "    with st.spinner(f\"Traveling back in time to {target_date_str}...\"):\n",
        "        decision_date_str = get_trading_day_before(target_date_input)\n",
        "        df_seq = get_historical_stock_data(decision_date_str)\n",
        "\n",
        "        if len(df_seq) < SEQ_LEN:\n",
        "            st.error(f\"Not enough data found ending on {decision_date_str}.\")\n",
        "        else:\n",
        "            KPI_COLS = ['Close', 'RSI', 'MACD', 'SMA_14', 'Return']\n",
        "            scaler = MinMaxScaler()\n",
        "            df_scaled = df_seq.copy()\n",
        "            df_scaled[KPI_COLS] = scaler.fit_transform(df_seq[KPI_COLS])\n",
        "\n",
        "            seq_kpis = df_scaled[KPI_COLS].tail(SEQ_LEN).values\n",
        "            kpi_tensor = torch.tensor(seq_kpis, dtype=torch.float).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "            model_loaded = False\n",
        "            DETECTED_NODES = 50\n",
        "\n",
        "            if os.path.exists(MODEL_FILE):\n",
        "                try:\n",
        "                    state_dict = torch.load(MODEL_FILE, map_location=DEVICE)\n",
        "\n",
        "                    DETECTED_NODES = state_dict['gnn1.lin.weight'].shape[1]\n",
        "\n",
        "                    model = IntegratedTrader(node_feat_dim=DETECTED_NODES, kpi_dim=len(KPI_COLS), lstm_hidden=128).to(DEVICE)\n",
        "                    model.load_state_dict(state_dict)\n",
        "                    model.eval()\n",
        "                    model_loaded = True\n",
        "                except Exception as e:\n",
        "                    st.sidebar.error(f\"Error loading brain: {e}\")\n",
        "                    model = IntegratedTrader(node_feat_dim=DETECTED_NODES, kpi_dim=len(KPI_COLS), lstm_hidden=128).to(DEVICE)\n",
        "            else:\n",
        "                st.warning(\"‚ö†Ô∏è Model not found. Using Random Weights.\")\n",
        "                model = IntegratedTrader(node_feat_dim=DETECTED_NODES, kpi_dim=len(KPI_COLS), lstm_hidden=128).to(DEVICE)\n",
        "\n",
        "            x = torch.eye(DETECTED_NODES)\n",
        "            edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long)\n",
        "            graph_snapshot = Data(x=x, edge_index=edge_index)\n",
        "            graph_seq = [graph_snapshot] * SEQ_LEN\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred_scaled = model(graph_seq, kpi_tensor).item() if model_loaded else 0.55\n",
        "\n",
        "            dummy = np.zeros((1, len(KPI_COLS)))\n",
        "            dummy[0, 0] = pred_scaled\n",
        "            price_pred = scaler.inverse_transform(dummy)[0, 0]\n",
        "\n",
        "            last_close = df_seq['Close'].iloc[-1]\n",
        "            change_pct = ((price_pred - last_close) / last_close) * 100\n",
        "\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "            col1.metric(\"Last Close\", f\"${last_close:.2f}\", f\"On {decision_date_str}\")\n",
        "            col2.metric(\"AI Target\", f\"${price_pred:.2f}\", f\"{change_pct:.2f}%\")\n",
        "\n",
        "            if price_pred > last_close * 1.005: signal, color = \"BUY \", \"green\"\n",
        "            elif price_pred < last_close * 0.995: signal, color = \"SELL \", \"red\"\n",
        "            else: signal, color = \"HOLD \", \"gray\"\n",
        "            col3.markdown(f\"### Signal: :{color}[{signal}]\")\n",
        "\n",
        "            fig = go.Figure()\n",
        "            plot_df = df_seq.tail(30)\n",
        "            fig.add_trace(go.Scatter(x=plot_df['Date'], y=plot_df['Close'], mode='lines', name='History', line=dict(color='#00d2be')))\n",
        "            fig.add_trace(go.Scatter(x=[target_date_str], y=[price_pred], mode='markers', name='AI Prediction', marker=dict(color='red' if 'SELL' in signal else 'green', size=14, symbol='star')))\n",
        "            fig.add_trace(go.Scatter(x=[plot_df['Date'].iloc[-1], target_date_str], y=[plot_df['Close'].iloc[-1], price_pred], mode='lines', line=dict(color='gray', dash='dot'), showlegend=False))\n",
        "            fig.update_layout(template=\"plotly_dark\", height=400)\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "else:\n",
        "    st.info(\" Select a date in the sidebar to start.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpeISKo0cphb"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "NGROK_TOKEN = \"37AWuu4MjUuB90VQK7gLkm3ptlK_2NCrw2uygxsntWCaZXs3e\"\n",
        "\n",
        "if NGROK_TOKEN == \"PASTE_YOUR_TOKEN_HERE\":\n",
        "    print(\" STOP! You didn't paste your token. Please do step 1 & 2 above.\")\n",
        "else:\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "\n",
        "    !nohup streamlit run app.py --server.port 8501 &\n",
        "\n",
        "    try:\n",
        "        public_url = ngrok.connect(8501).public_url\n",
        "        print(f\" Your App is Live: {public_url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}