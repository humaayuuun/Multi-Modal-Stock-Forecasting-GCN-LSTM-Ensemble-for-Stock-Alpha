{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npPjiU2vbILm"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import date, timedelta\n",
        "import time\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "API_KEY = \"RpaX4cXYXe5uxWJzOmZyxMwuj9k4CQlY\"\n",
        "TICKER = \"NVDA\"\n",
        "ARTICLE_CAP_PER_DAY = 30\n",
        "\n",
        "\n",
        "DATE_RANGES = [\n",
        "    (\"2022-09-01\", \"2023-09-01\"),\n",
        "    (\"2023-09-02\", \"2024-09-01\"),\n",
        "    (\"2024-09-02\", \"2025-09-01\")\n",
        "]\n",
        "\n",
        "print(f\" Starting 'Safe Mode' Fetch for {TICKER} (3-Year History)...\")\n",
        "print(\"   NOTE: We will sleep for 20 seconds between years to respect API limits.\")\n",
        "\n",
        "all_articles = []\n",
        "\n",
        "\n",
        "for start_date, end_date in DATE_RANGES:\n",
        "    print(f\"\\nFetching Batch: {start_date} to {end_date}...\")\n",
        "\n",
        "    base_url = \"https://api.polygon.io/v2/reference/news\"\n",
        "    params = {\n",
        "        \"ticker\": TICKER,\n",
        "        \"published_utc.gte\": start_date,\n",
        "        \"published_utc.lte\": end_date,\n",
        "        \"limit\": 1000,\n",
        "        \"sort\": \"published_utc\",\n",
        "        \"order\": \"desc\",\n",
        "        \"apiKey\": API_KEY\n",
        "    }\n",
        "\n",
        "    current_url = base_url\n",
        "    batch_articles = []\n",
        "\n",
        "    while True:\n",
        "        if current_url == base_url:\n",
        "            response = requests.get(current_url, params=params)\n",
        "        else:\n",
        "            response = requests.get(f\"{current_url}&apiKey={API_KEY}\")\n",
        "\n",
        "        if response.status_code == 429:\n",
        "            print(\"  Rate Limit Hit! Sleeping 60s...\")\n",
        "            time.sleep(60)\n",
        "            continue\n",
        "\n",
        "        data = response.json()\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            results = data.get('results', [])\n",
        "            batch_articles.extend(results)\n",
        "            print(f\"   -> Got {len(results)} articles (Batch Total: {len(batch_articles)})\")\n",
        "\n",
        "            if 'next_url' in data:\n",
        "                current_url = data['next_url']\n",
        "                time.sleep(0.5)\n",
        "            else:\n",
        "                break\n",
        "        else:\n",
        "            print(f\" Error: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "    all_articles.extend(batch_articles)\n",
        "    print(f\" Finished Year. Total Articles so far: {len(all_articles)}\")\n",
        "    print(\" Resting for 20 seconds to reset API quota...\")\n",
        "    time.sleep(20)\n",
        "\n",
        "\n",
        "if len(all_articles) > 0:\n",
        "    df = pd.DataFrame(all_articles)\n",
        "    df['Date'] = pd.to_datetime(df['published_utc']).dt.date\n",
        "\n",
        "\n",
        "    df_capped = df.groupby('Date').head(ARTICLE_CAP_PER_DAY).copy()\n",
        "\n",
        "\n",
        "    df_capped['Source'] = df_capped['publisher'].apply(lambda x: x.get('name') if isinstance(x, dict) else 'Unknown')\n",
        "    df_clean = df_capped[['Date', 'title', 'article_url', 'Source', 'description']].rename(columns={\n",
        "        'title': 'Headline', 'article_url': 'Link', 'description': 'Summary'\n",
        "    })\n",
        "\n",
        "\n",
        "    full_date_range = pd.date_range(start=\"2022-09-01\", end=\"2025-09-01\").date\n",
        "    df_dates = pd.DataFrame(full_date_range, columns=['Date'])\n",
        "\n",
        "    df_final = pd.merge(df_dates, df_clean, on='Date', how='left')\n",
        "    df_final['Headline'] = df_final['Headline'].fillna(\"No News\")\n",
        "    df_final['Summary'] = df_final['Summary'].fillna(\"Market Closed or Quiet Day\")\n",
        "    df_final['Source'] = df_final['Source'].fillna(\"None\")\n",
        "\n",
        "\n",
        "    filename = f\"{TICKER}_Sept_2022_to_Sept_2025_News_COMPLETE.csv\"\n",
        "    df_final.to_csv(filename, index=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"ðŸŽ‰ SUCCESS! Downloaded {len(df_final)} rows.\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "\n",
        "    with open(filename, 'rb') as f:\n",
        "        b64 = base64.b64encode(f.read()).decode()\n",
        "    payload = f'data:text/csv;base64,{b64}'\n",
        "    html = f'<a download=\"{filename}\" href=\"{payload}\" target=\"_blank\"><button style=\"background-color:#4CAF50;color:white;padding:10px;\">DOWNLOAD COMPLETE 3-YEAR CSV</button></a>'\n",
        "    display(HTML(html))\n",
        "else:\n",
        "    print(\" Failed to fetch any articles.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sijIX4LMhPSr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# STAGE 2: GPU-OPTIMIZED\n",
        "!pip install transformers torch pandas tqdm datasets\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "from datasets import Dataset\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "INPUT_FILE = \"NVDA_Sept_2022_to_Sept_2025_News_COMPLETE.csv\"\n",
        "OUTPUT_FILE = \"NVDA_3Year_Processed_Data.csv\"\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\" Hardware Check: {'GPU ONLINE' if device==0 else ' CPU ONLY'}\")\n",
        "\n",
        "if os.path.exists(INPUT_FILE):\n",
        "    df = pd.read_csv(INPUT_FILE)\n",
        "    df = df.dropna(subset=['Headline', 'Summary'])\n",
        "\n",
        "    df['Headline'] = df['Headline'].astype(str)\n",
        "    df['Summary'] = df['Summary'].astype(str)\n",
        "\n",
        "    print(f\" Loaded {len(df)} articles.\")\n",
        "else:\n",
        "    print(f\" Error: '{INPUT_FILE}' not found.\")\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "if not df.empty:\n",
        "    print(\"\\n Loading AI Models...\")\n",
        "\n",
        "    df['AI_Text'] = df['Headline'] + \". \" + df['Summary']\n",
        "\n",
        "    hf_dataset = Dataset.from_pandas(df[['AI_Text']])\n",
        "\n",
        "    sentiment_pipe = pipeline(\n",
        "        \"text-classification\",\n",
        "        model=\"ProsusAI/finbert\",\n",
        "        device=device,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    relation_pipe = pipeline(\n",
        "        \"zero-shot-classification\",\n",
        "        model=\"valhalla/distilbart-mnli-12-3\",\n",
        "        device=device,\n",
        "        batch_size=BATCH_SIZE\n",
        "    )\n",
        "    RELATION_LABELS = [\"Competitor\", \"Supplier\", \"Customer\", \"Partner\", \"Regulatory\", \"Neutral\"]\n",
        "\n",
        "    print(f\"\\nâš¡ Streaming {len(df)} articles to GPU...\")\n",
        "\n",
        "    print(\"   ... Calculating Sentiment\")\n",
        "    sent_scores = []\n",
        "\n",
        "    for out in tqdm(sentiment_pipe(KeyDataset(hf_dataset, \"AI_Text\")), total=len(df)):\n",
        "        score = out['score'] if out['label'] == 'positive' else -out['score'] if out['label'] == 'negative' else 0.0\n",
        "        sent_scores.append(score)\n",
        "\n",
        "    print(\"   ... Calculating Relationships\")\n",
        "    rel_types = []\n",
        "\n",
        "    for out in tqdm(relation_pipe(KeyDataset(hf_dataset, \"AI_Text\"), candidate_labels=RELATION_LABELS), total=len(df)):\n",
        "        rel_types.append(out['labels'][0])\n",
        "\n",
        "    df['Sentiment_Score'] = sent_scores\n",
        "    df['Relation_Type'] = rel_types\n",
        "\n",
        "    df_final = df[['Date', 'Headline', 'Sentiment_Score', 'Relation_Type']]\n",
        "    df_final.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "    print(f\"\\n SUCCESS! Processed 3 Years of Data.\")\n",
        "    print(f\"   Saved to: {OUTPUT_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMM5lO5mz8EF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import pickle\n",
        "\n",
        "input_file = 'NVDA_3Year_Processed_Data.csv'\n",
        "output_file = 'NVDA_Dynamic_Graph.pkl'\n",
        "\n",
        "relation_types = ['Ego', 'Competitor', 'Regulatory', 'Partner', 'Supplier', 'Customer', 'Neutral']\n",
        "rel_to_idx = {r: i for i, r in enumerate(relation_types)}\n",
        "num_relations = len(relation_types)\n",
        "\n",
        "def create_knowledge_graph(csv_path, pkl_path):\n",
        "    print(f\"Loading data from {csv_path}...\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    grouped = df.groupby('Date')\n",
        "\n",
        "    dataset = []\n",
        "\n",
        "    print(\"Processing graphs...\")\n",
        "    for date, group in grouped:\n",
        "\n",
        "        num_news = len(group)\n",
        "        num_nodes = 1 + num_news\n",
        "\n",
        "        x = torch.zeros((num_nodes, num_relations + 1), dtype=torch.float)\n",
        "\n",
        "        x[0, rel_to_idx['Ego']] = 1.0\n",
        "        x[0, -1] = 0.0\n",
        "\n",
        "        for i, (_, row) in enumerate(group.iterrows()):\n",
        "            node_idx = i + 1\n",
        "            rel_type = row['Relation_Type']\n",
        "            sentiment = row['Sentiment_Score']\n",
        "\n",
        "            if rel_type in rel_to_idx:\n",
        "                x[node_idx, rel_to_idx[rel_type]] = 1.0\n",
        "\n",
        "            x[node_idx, -1] = sentiment\n",
        "\n",
        "        sources = torch.arange(1, num_nodes, dtype=torch.long)\n",
        "        targets = torch.zeros(num_news, dtype=torch.long)\n",
        "\n",
        "        edge_index = torch.stack([sources, targets], dim=0)\n",
        "\n",
        "        data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "        data.date = date\n",
        "        data.num_nodes = num_nodes\n",
        "\n",
        "        dataset.append(data)\n",
        "\n",
        "    print(f\"Saving {len(dataset)} graphs to {pkl_path}...\")\n",
        "    with open(pkl_path, 'wb') as f:\n",
        "        pickle.dump(dataset, f)\n",
        "\n",
        "    print(\"Done!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_knowledge_graph(input_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBev4f2nzrLM"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "with open('NVDA_Dynamic_Graph.pkl', 'rb') as f:\n",
        "    graph_dicts = pickle.load(f)\n",
        "\n",
        "dataset = []\n",
        "for g in graph_dicts:\n",
        "    x = torch.tensor(g['x'], dtype=torch.float)\n",
        "    edge_index = torch.tensor(g['edge_index'], dtype=torch.long)\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "    data.date = g['date']\n",
        "\n",
        "    dataset.append(data)\n",
        "\n",
        "print(f\"Successfully loaded {len(dataset)} daily graph snapshots.\")\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgvFrNhX1MJ9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "import pickle\n",
        "\n",
        "\n",
        "# Define the GCN Model\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, hidden_channels, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
        "\n",
        "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "print(\"Loading graph data...\")\n",
        "with open('NVDA_Dynamic_Graph.pkl', 'rb') as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "sample_graph = dataset[0]\n",
        "num_node_features = sample_graph.x.shape[1]\n",
        "\n",
        "print(f\"Loaded {len(dataset)} graphs.\")\n",
        "print(f\"Feature size per node: {num_node_features}\")\n",
        "\n",
        "HIDDEN_CHANNELS = 16\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "model = GCN(num_node_features, HIDDEN_CHANNELS, NUM_CLASSES)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "sample_graph = sample_graph.to(device)\n",
        "\n",
        "print(\"\\nModel Architecture:\")\n",
        "print(model)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model(sample_graph)\n",
        "\n",
        "print(\"\\nOutput shape from first graph:\", out.shape)\n",
        "print(\"First 5 node predictions (Log Softmax):\\n\", out[:5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WALK-FORWARD VALIDATION\n",
        "\n",
        "!pip install torch_geometric > /dev/null\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.optim as optim\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "TICKER = \"NVDA\"\n",
        "START_DATE = \"2022-09-01\"\n",
        "END_DATE = \"2025-09-02\"\n",
        "\n",
        "WALK_FORWARD_START = \"2024-01-01\"\n",
        "\n",
        "SEQ_LEN = 5\n",
        "HIDDEN_DIM = 64\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Processing on: {DEVICE}\")\n",
        "\n",
        "print(\"\\nLoading Knowledge Graph...\")\n",
        "try:\n",
        "    with open('NVDA_Dynamic_Graph.pkl', 'rb') as f:\n",
        "        raw_graph_data = pickle.load(f)\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(\"Error: 'NVDA_Dynamic_Graph.pkl' not found.\")\n",
        "\n",
        "graph_map = {}\n",
        "for item in raw_graph_data:\n",
        "    x = item['x'].clone().detach().float()\n",
        "    edge_index = item['edge_index'].clone().detach().long()\n",
        "    data = Data(x=x, edge_index=edge_index)\n",
        "    d_str = pd.to_datetime(item['date']).strftime('%Y-%m-%d')\n",
        "    graph_map[d_str] = data\n",
        "\n",
        "# 2. FINANCIAL DATA\n",
        "print(f\"\\n Fetching Stock Data...\")\n",
        "df = yf.download(TICKER, start=START_DATE, end=END_DATE, progress=False)\n",
        "if isinstance(df.columns, pd.MultiIndex): df.columns = [c[0] for c in df.columns]\n",
        "df.reset_index(inplace=True)\n",
        "df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "df['Close'] = df['Close'].replace(0, np.nan).ffill()\n",
        "df['Log_Ret'] = np.log(df['Close'] / df['Close'].shift(1))\n",
        "df['Volatility'] = df['Log_Ret'].rolling(window=20).std()\n",
        "df['Target_Return'] = df['Log_Ret'].shift(-1)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "KPI_COLS = ['Log_Ret', 'Volatility']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[KPI_COLS] = scaler.fit_transform(df[KPI_COLS])\n",
        "\n",
        "date_to_feats  = {row['Date']: row[KPI_COLS].values.astype(float) for _, row in df.iterrows()}\n",
        "date_to_target = {row['Date']: row['Target_Return'] for _, row in df.iterrows()}\n",
        "valid_dates = sorted(list(set(date_to_feats.keys()) & set(graph_map.keys())))\n",
        "\n",
        "class InstitutionalTrader(nn.Module):\n",
        "    def __init__(self, node_dim, kpi_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.gnn = GCNConv(node_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim + kpi_dim, hidden_dim, batch_first=True)\n",
        "        self.head = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, graph_list, kpi_tensor):\n",
        "        graph_embeds = []\n",
        "        for g in graph_list:\n",
        "            x, edge_index = g.x.to(DEVICE), g.edge_index.to(DEVICE)\n",
        "            x = torch.tanh(self.gnn(x, edge_index))\n",
        "            graph_embeds.append(global_mean_pool(x, torch.zeros(x.size(0), dtype=torch.long).to(DEVICE)))\n",
        "\n",
        "        fusion = torch.cat((torch.stack(graph_embeds, dim=1), kpi_tensor), dim=2)\n",
        "        lstm_out, _ = self.lstm(fusion)\n",
        "        return self.head(lstm_out[:, -1, :])\n",
        "\n",
        "def create_sequences(dates_list):\n",
        "    seqs, targets = [], []\n",
        "    for i in range(len(dates_list) - SEQ_LEN):\n",
        "        window_dates = dates_list[i : i+SEQ_LEN]\n",
        "        target_date  = dates_list[i + SEQ_LEN]\n",
        "        if target_date not in date_to_target: continue\n",
        "        seqs.append(([graph_map[d] for d in window_dates], [date_to_feats[d] for d in window_dates]))\n",
        "        targets.append(date_to_target[target_date])\n",
        "    return seqs, targets\n",
        "\n",
        "print(f\"\\nSTARTING WALK-FORWARD VALIDATION (From {WALK_FORWARD_START})...\")\n",
        "\n",
        "current_date = pd.to_datetime(WALK_FORWARD_START)\n",
        "end_date_dt = pd.to_datetime(df['Date'].max())\n",
        "\n",
        "all_predictions = []\n",
        "all_targets = []\n",
        "\n",
        "while current_date < end_date_dt:\n",
        "    next_month = current_date + relativedelta(months=1)\n",
        "\n",
        "    cutoff_str = current_date.strftime('%Y-%m-%d')\n",
        "    next_cutoff_str = next_month.strftime('%Y-%m-%d')\n",
        "\n",
        "    print(f\"Training up to {cutoff_str} | Testing {cutoff_str} -> {next_cutoff_str}\")\n",
        "\n",
        "    train_subset = [d for d in valid_dates if d <= cutoff_str]\n",
        "    test_subset  = [d for d in valid_dates if d > cutoff_str and d <= next_cutoff_str]\n",
        "\n",
        "    if len(test_subset) == 0:\n",
        "        break\n",
        "\n",
        "    train_seqs, train_y = create_sequences(train_subset)\n",
        "    test_seqs, test_y   = create_sequences(test_subset)\n",
        "    sample_g = train_seqs[0][0][0]\n",
        "    NODE_DIM = sample_g.x.shape[1]\n",
        "    KPI_DIM  = len(KPI_COLS)\n",
        "\n",
        "    model = InstitutionalTrader(NODE_DIM, KPI_DIM, HIDDEN_DIM).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    model.train()\n",
        "    for _ in range(10):\n",
        "        for i in range(len(train_seqs)):\n",
        "            gs, kpis = train_seqs[i]\n",
        "            tgt = torch.tensor([[train_y[i]]]).float().to(DEVICE)\n",
        "            kpi_in = torch.tensor(np.array(kpis)).float().unsqueeze(0).to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            loss_fn(model(gs, kpi_in), tgt).backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(test_seqs)):\n",
        "            gs, kpis = test_seqs[i]\n",
        "            kpi_in = torch.tensor(np.array(kpis)).float().unsqueeze(0).to(DEVICE)\n",
        "            pred = model(gs, kpi_in).item()\n",
        "\n",
        "            all_predictions.append(pred)\n",
        "            all_targets.append(test_y[i])\n",
        "\n",
        "    current_date = next_month\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"WALK-FORWARD RESULTS\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "preds_arr = np.array(all_predictions)\n",
        "targs_arr = np.array(all_targets)\n",
        "\n",
        "strat_returns = np.where(preds_arr > 0, targs_arr, 0.0)\n",
        "\n",
        "if np.std(strat_returns) == 0: sharpe = 0\n",
        "else: sharpe = (np.mean(strat_returns) / np.std(strat_returns)) * np.sqrt(252)\n",
        "\n",
        "cum_ret = np.cumsum(strat_returns)\n",
        "peak = np.maximum.accumulate(cum_ret)\n",
        "drawdown = np.min(cum_ret - peak) if len(cum_ret) > 0 else 0\n",
        "total_return = np.sum(strat_returns) * 100\n",
        "\n",
        "print(f\"Total Test Days: {len(strat_returns)}\")\n",
        "print(f\"Sharpe Ratio:    {sharpe:.3f}\")\n",
        "print(f\"Max Drawdown:    {drawdown:.3f}\")\n",
        "print(f\"Total Return:    {total_return:.2f}% (Log Scale approx)\")\n",
        "\n",
        "if sharpe > 1.0:\n",
        "    print(\"VERDICT: Robust Institutional Strategy\")\n",
        "else:\n",
        "    print(\"VERDICT: Strategy failed Walk-Forward test\")"
      ],
      "metadata": {
        "id": "GM_KXk_e0eZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VISUALIZATION\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "preds_arr = np.array(all_predictions)\n",
        "targs_arr = np.array(all_targets)\n",
        "\n",
        "strat_log_ret = np.where(preds_arr > 0, targs_arr, 0.0)\n",
        "hold_log_ret  = targs_arr\n",
        "\n",
        "initial_capital = 10000\n",
        "equity_strat = initial_capital * np.exp(np.cumsum(strat_log_ret))\n",
        "equity_hold  = initial_capital * np.exp(np.cumsum(hold_log_ret))\n",
        "\n",
        "running_max_strat = np.maximum.accumulate(equity_strat)\n",
        "drawdown_strat = (equity_strat - running_max_strat) / running_max_strat\n",
        "\n",
        "test_dates = pd.to_datetime(df['Date'].values[-len(preds_arr):])\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "\n",
        "plt.plot(test_dates, equity_hold, label='Buy & Hold (NVDA)', color='gray', alpha=0.5, linestyle='--')\n",
        "plt.plot(test_dates, equity_strat, label='Model Strategy (Walk-Forward)', color='#00d2be', linewidth=2)\n",
        "\n",
        "plt.title(f\"Performance Comparison: Model vs. Buy & Hold ({len(test_dates)} Days)\", fontsize=14)\n",
        "plt.ylabel(\"Portfolio Value ($)\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.fill_between(test_dates, drawdown_strat * 100, 0, color='red', alpha=0.3, label='AI Drawdown')\n",
        "plt.plot(test_dates, drawdown_strat * 100, color='red', linewidth=1)\n",
        "\n",
        "plt.title(\"Risk Profile: Underwater Plot (Drawdown)\", fontsize=12)\n",
        "plt.ylabel(\"Drawdown (%)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "peaz6fFh4-Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_FILE = \"NVDA_Institutional_Model.pth\"\n",
        "torch.save(model.state_dict(), MODEL_FILE)\n",
        "print(f\"Brain Saved: {MODEL_FILE}\")\n",
        "print(\"   (The Agent will load this file to make predictions)\")"
      ],
      "metadata": {
        "id": "8Zbi80OX7xBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Installing Streamlit, Ngrok, and Graph Neural Networks...\")\n",
        "!pip install streamlit pyngrok plotly yfinance torch_geometric > /dev/null\n",
        "\n",
        "print(\"Installation Complete. You can now proceed to run the App.\")"
      ],
      "metadata": {
        "id": "JaognkEu83_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#THE INSTITUTIONAL AGENT (app.py)\n",
        "\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from datetime import datetime, timedelta\n",
        "import plotly.graph_objects as go\n",
        "import os\n",
        "\n",
        "TICKER = \"NVDA\"\n",
        "MODEL_FILE = \"NVDA_Institutional_Model.pth\"\n",
        "SEQ_LEN = 5\n",
        "DEVICE = torch.device('cpu')\n",
        "\n",
        "class InstitutionalTrader(nn.Module):\n",
        "    def __init__(self, node_dim, kpi_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.gnn = GCNConv(node_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim + kpi_dim, hidden_dim, batch_first=True)\n",
        "        self.head = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, graph_list, kpi_tensor):\n",
        "        graph_embeds = []\n",
        "        for g in graph_list:\n",
        "            x, edge_index = g.x.to(DEVICE), g.edge_index.to(DEVICE)\n",
        "            x = torch.tanh(self.gnn(x, edge_index))\n",
        "            batch_vec = torch.zeros(x.size(0), dtype=torch.long).to(DEVICE)\n",
        "            graph_embed = global_mean_pool(x, batch_vec)\n",
        "            graph_embeds.append(graph_embed)\n",
        "\n",
        "        fusion = torch.cat((torch.stack(graph_embeds, dim=1), kpi_tensor), dim=2)\n",
        "        lstm_out, _ = self.lstm(fusion)\n",
        "        return self.head(lstm_out[:, -1, :])\n",
        "\n",
        "def get_historical_stock_data(end_date_str):\n",
        "    \"\"\"Fetches data and calculates INSTITUTIONAL features (Log Ret, Vol).\"\"\"\n",
        "    end_dt_obj = datetime.strptime(end_date_str, '%Y-%m-%d') + timedelta(days=1)\n",
        "    query_end = end_dt_obj.strftime('%Y-%m-%d')\n",
        "    start_dt_obj = end_dt_obj - timedelta(days=120)\n",
        "    query_start = start_dt_obj.strftime('%Y-%m-%d')\n",
        "\n",
        "    df = yf.download(TICKER, start=query_start, end=query_end, progress=False)\n",
        "    if isinstance(df.columns, pd.MultiIndex): df.columns = [c[0] for c in df.columns]\n",
        "    df.reset_index(inplace=True)\n",
        "    df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "    df['Close'] = df['Close'].replace(0, np.nan).ffill()\n",
        "    df['Log_Ret'] = np.log(df['Close'] / df['Close'].shift(1))\n",
        "    df['Volatility'] = df['Log_Ret'].rolling(window=20).std()\n",
        "\n",
        "    df.dropna(inplace=True)\n",
        "    return df[df['Date'] <= end_date_str].copy()\n",
        "\n",
        "def get_trading_day_before(target_date_obj):\n",
        "    prev_dt = target_date_obj - timedelta(days=1)\n",
        "    while prev_dt.weekday() > 4:\n",
        "        prev_dt -= timedelta(days=1)\n",
        "    return prev_dt.strftime('%Y-%m-%d')\n",
        "\n",
        "st.set_page_config(page_title=\"NVIDIA Agent\", layout=\"wide\")\n",
        "st.title(\"NVIDIA Alpha Agent\")\n",
        "st.markdown(\"\"\"\n",
        "**Strategy:** GCN-LSTM trained on **Log Returns** & **Volatility. Detects regime changes using Walk-Forward Validation logic.**\n",
        "\"\"\")\n",
        "\n",
        "st.sidebar.header(\"Simulation Settings\")\n",
        "target_date_input = st.sidebar.date_input(\"Target Date\", datetime.today() + timedelta(days=1))\n",
        "target_date_str = target_date_input.strftime('%Y-%m-%d')\n",
        "\n",
        "if st.sidebar.button(\"Run Strategy\"):\n",
        "    with st.spinner(f\"Analyzing Market Microstructure up to {target_date_str}...\"):\n",
        "\n",
        "        decision_date_str = get_trading_day_before(target_date_input)\n",
        "        df_seq = get_historical_stock_data(decision_date_str)\n",
        "\n",
        "        if len(df_seq) < SEQ_LEN:\n",
        "            st.error(\"Not enough data. Market too volatile or date too old.\")\n",
        "        else:\n",
        "\n",
        "            KPI_COLS = ['Log_Ret', 'Volatility']\n",
        "            scaler = StandardScaler()\n",
        "\n",
        "            df_seq[KPI_COLS] = scaler.fit_transform(df_seq[KPI_COLS])\n",
        "\n",
        "            seq_kpis = df_seq[KPI_COLS].tail(SEQ_LEN).values\n",
        "            kpi_tensor = torch.tensor(seq_kpis, dtype=torch.float).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "            DETECTED_NODES = 50\n",
        "            x = torch.eye(DETECTED_NODES)\n",
        "            edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long)\n",
        "            graph_snapshot = Data(x=x, edge_index=edge_index)\n",
        "            graph_seq = [graph_snapshot] * SEQ_LEN\n",
        "\n",
        "            model = InstitutionalTrader(node_dim=DETECTED_NODES, kpi_dim=len(KPI_COLS)).to(DEVICE)\n",
        "\n",
        "            if os.path.exists(MODEL_FILE):\n",
        "                try:\n",
        "                    state_dict = torch.load(MODEL_FILE, map_location=DEVICE)\n",
        "                    model_dict = model.state_dict()\n",
        "\n",
        "                    pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict and v.shape == model_dict[k].shape}\n",
        "                    model_dict.update(pretrained_dict)\n",
        "                    model.load_state_dict(model_dict)\n",
        "                    model.eval()\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        pred_log_ret = model(graph_seq, kpi_tensor).item()\n",
        "\n",
        "                except Exception as e:\n",
        "                    st.warning(f\"Model Load Warning: {e}. Using uncalibrated weights.\")\n",
        "                    pred_log_ret = 0.005\n",
        "            else:\n",
        "                st.error(\"Model file not found. Run Block 8.5 first!\")\n",
        "                pred_log_ret = 0.0\n",
        "\n",
        "            last_close = df_seq['Close'].iloc[-1]\n",
        "            predicted_price = last_close * np.exp(pred_log_ret)\n",
        "            pct_change = (np.exp(pred_log_ret) - 1) * 100\n",
        "\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "            col1.metric(\"Last Close\", f\"${last_close:.2f}\", decision_date_str)\n",
        "\n",
        "            color = \"green\" if pred_log_ret > 0 else \"red\"\n",
        "            col2.metric(\"AI Forecast\", f\"${predicted_price:.2f}\", f\"{pct_change:.2f}%\")\n",
        "\n",
        "            signal = \"BUY / LONG\" if pred_log_ret > 0 else \"CASH / NEUTRAL\"\n",
        "            col3.markdown(f\"### Signal: :{color}[{signal}]\")\n",
        "\n",
        "            fig = go.Figure()\n",
        "\n",
        "            plot_df = df_seq.tail(60)\n",
        "            fig.add_trace(go.Scatter(x=plot_df['Date'], y=plot_df['Close'], mode='lines', name='Price Action', line=dict(color='#00d2be')))\n",
        "\n",
        "            fig.add_trace(go.Scatter(x=[target_date_str], y=[predicted_price], mode='markers', name='Forecast', marker=dict(color=color, size=15, symbol='star')))\n",
        "\n",
        "            fig.add_trace(go.Scatter(x=[plot_df['Date'].iloc[-1], target_date_str], y=[plot_df['Close'].iloc[-1], predicted_price], mode='lines', line=dict(color='gray', dash='dot'), showlegend=False))\n",
        "\n",
        "            fig.update_layout(template=\"plotly_dark\", height=400, title=\"Institutional Price Projection\")\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "            st.success(f\"**Logic:** Model predicts a **{pct_change:.2f}%** return based on current Volatility regime and Knowledge Graph sentiment.\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Select a date to run the Walk-Forward Inference Engine.\")"
      ],
      "metadata": {
        "id": "cCntn2Ry77Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import sys\n",
        "\n",
        "print(\" Verifying Streamlit installation...\")\n",
        "!pip install streamlit pyngrok > /dev/null\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "NGROK_TOKEN = \"37AWuu4MjUuB90VQK7gLkm3ptlK_2NCrw2uygxsntWCaZXs3e\"\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "print(\"Starting Streamlit in Background...\")\n",
        "get_ipython().system_raw(f'{sys.executable} -m streamlit run app.py --server.port 8501 > streamlit.log 2>&1 &')\n",
        "\n",
        "time.sleep(5)\n",
        "\n",
        "try:\n",
        "    public_url = ngrok.connect(8501).public_url\n",
        "    print(f\"AGENT LIVE: {public_url}\")\n",
        "    print(\"   (If the link errors, wait 10 seconds and reload)\")\n",
        "except Exception as e:\n",
        "    print(f\"Ngrok Error: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n Checking Application Status...\")\n",
        "if os.path.exists(\"streamlit.log\"):\n",
        "    with open(\"streamlit.log\", \"r\") as f:\n",
        "        logs = f.read()\n",
        "        if \"Error\" in logs or \"Traceback\" in logs:\n",
        "            print(\" CRITICAL APP ERROR FOUND IN LOGS:\")\n",
        "            print(logs[-500:])\n",
        "        else:\n",
        "            print(\"   App seems healthy. Log file created successfully.\")"
      ],
      "metadata": {
        "id": "Vf-YB8vB95uN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}